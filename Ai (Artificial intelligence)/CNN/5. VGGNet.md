# ILSVRC
- 이미지넷의 사물 인식 대회

```
2012
- AlexNet

2013
- ZFNet

2014
- GoogleNet
- VGGNet

2015
- ResNet

2016
- GoogleNet-v4

2017
- SENet
```

# VGGNet
- 2014 이미지 사물인색 대회에서 2위
- VGGNet-16
	- 13 conv layers + 3 FC
	- 138 million weights and 500MB of storage space in total
	- 변형: VGGNet-19
- VGGNet 이전에 나왔던 AlexNet의 경우에는 첫번째 Conv layer 11X11필터를 사용
- VGGNet의 가장 큰 특징
	-  3X3 필터만 사용해도 충분하다는 것을 보여줌

- Local Connectivity
	- Receptive Field(RF) - 수용할 수 있는 영역
	- Kernel 이 Convolution 연산을 수행할 때 이미지의 얼마나 많은 영역을 수용할 수 있는가?

![[Pasted image 20240301160711.png]]

- Convolution 연산을 수행할 수록 RF가 더 커짐
![[Pasted image 20240301160803.png]]

- 7X7 커널을 하나만 사용한 경우 vs 3X3커널을 3번 사용한 경우
	- 동일한 효과를 갖음
	- Conv연산의 횟수가 증가하는데 사용하는 메모리가 늘어나는 단점이 있지 않을까?

![[Pasted image 20240301160903.png]]

- 5X5 필터를 사용하는 경우
	- layer i의 가중치 수 = 학습을 통해 업데이터 되는 파라미터의 수
	-  2번째 레이어의 weight의 수 = ( 5X5X3 ) X 1 = 75

- 3X3필터를 두 번 사용하는 경우
	- layer i 의 가중치 수 = 학습을 통해 업데이트 되는 파라미터의 수
	- 2번째 레이어의 가중치의 수 = ( 3X3X3 ) X 1 = 27
	- 3번째 레이어의 가중치의 수 = ( 3X3X1 ) X 1 = 9
	- 총 weight의 수 = 36

- 5X5 필터를 사용하는 경우 vs 3X3필터를 두 번 사용하는 경우
	- 동일한 RF
	- weight의 수 감소 ( 메모리 사용량 감소 )
	- 더 많은 Activation Function을 통과 ( 비선형성이 증가 )

# 정리
- VGGNet
	- 3X3 convolution연산을 여러 번 사용했을 때의 효과
	1. Receptive Filed를 늘리는 효과
	2. 큰 사이즈의 kernel을 사용했을 때 보다 Weight 의 수는 감소
	3. 더 많은 Activation Function 통과 