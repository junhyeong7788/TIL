# ğŸš€ í•™ìŠµ í‚¤ì›Œë“œ

## ì •ì˜

- AutoML : ë¨¸ì‹ ëŸ¬ë‹ì— í•„ìš”í•œ ë°ì´í„° ì „ì²˜ë¦¬, ëª¨ë¸ íŠœë‹, í•™ìŠµ ë“± ì—¬ëŸ¬ ìˆ˜í–‰ì‚¬í•­ë“¤ì„ ì‰½ê³  ê°„ë‹¨í•˜ê²Œ ì½”ë“œ ëª‡ ì¤„ì„ í†µí•´ ìë™í™”í•˜ëŠ” ë„êµ¬

## í‚¤ì›Œë“œ

- `AutoML ë„êµ¬`, `MLJARì—ì„œ ë§Œë“  mljar-supervised`

---

# ğŸ“ìƒˆë¡œ ë°°ìš´ ê°œë…

## mljar-supervised

- MLJAR íŒ€ì—ì„œ ë§Œë“  automl íˆ´ (mljar-supervised)
- Feature Engineeringì„ í•´ì£¼ë©° ê²°ê³¼ì— ëŒ€í•œ ë¶„ì„ ë° ì‹œê°í™”ê¹Œì§€ í•´ì¤€ë‹¤ëŠ” ì¥ì 

### mljarì˜ í•™ìŠµ ë°©ì‹

- `baseline`ì„ ë‘ê³  mlë¡œ ì˜ˆì¸¡ì´ ê°€ëŠ¥í•œ ë°ì´í„°ì¸ ì§€ë¥¼ í™•ì¸

  - ë¬´ì§€ì„± ì˜ˆì¸¡ì„ í–ˆì„ ë•Œ ê²°ê³¼ë¥¼ `baseline`ìœ¼ë¡œ ë‘ 
  - loglossëŠ” ì‘ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸ì¸ë°, `baseline`ì˜ ì„±ëŠ¥ì´ ê°€ì¥ ë›°ì–´ë‚˜ë‹¤.
    - MLë¡œ ì˜ˆì¸¡ì´ ë˜ì§€ ì•ŠëŠ” ë°ì´í„°ë¡œ ê°€ì • ìˆ˜ë¦½ê³¼ ë°ì´í„° ìˆ˜ì§‘ë¶€í„° ë‹¤ì‹œ ìˆ˜í–‰í•´ì•¼ í•œë‹¤ëŠ” ì˜ë¯¸

- `MODE`

  - Explain : ë°ì´í„° ì´í•´
  - Perform : ë°°í¬ë¥¼ ìœ„í•œ í•™ìŠµ
  - Compete : ëŒ€íšŒ ì°¸ê°€ë¥¼ ìœ„í•œ í•™ìŠµ

- `Algorithm`
  - ëœë¤ í¬ë ˆìŠ¤íŠ¸, ExtraTrees, XGBoost, LightGBM, CatBoost, KNN, DNN ë° ì•™ìƒë¸” ëª¨ë¸

## ì„¤ì¹˜

- `pip install mljar-supervised`
- `conda install -c conda-forge mljar-supervised`

## MODE

### Explain (default)

- ì²˜ìŒ ë°ì´í„°ë¥¼ í™•ì¸í•˜ê³  ë¶„ì„í• ë•Œ ì ì ˆí•˜ê²Œ ì‚¬ìš© ê°€ëŠ¥
  - 75:25 train/test split
  - Baseline, Linear, Decision Tree, Random Forest, Xgboost, Neural Network ëª¨ë¸ì„ ì‚¬ìš©
  - Reportì— learning curve, importance plot, SHAP plotì„ ì œê³µ

### Perform

- 5-fold CV
- Linear, RandomForest, LightGBM, Xgboost, CatBoost, Neural Network ëª¨ë¸ì„ ì‚¬ìš©
- Reportì— learning curveì™€ importance plotì„ ì œê³µ

- 1ì°¨ í•™ìŠµ
  - ê¸°ë³¸ ì•Œê³ ë¦¬ì¦˜ë“¤ë¡œ í•™ìŠµì„ ì§„í–‰
  - í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ (ëœë¤ ì„œì¹˜)
- Feature Engineering
  - Golden Features
    - ëª¨ë“  feature ì¤‘ í•œ ìŒì„ ê³¨ë¼ ê°€ëŠ¥í•œ ëª¨ë“  ì—°ì‚°ì— ëŒ€í•œ ìƒˆë¡œìš´ feature ìƒì„±
    - ê° featureë§Œìœ¼ë¡œ ì˜ˆì¸¡ ëª¨ë¸ì„ ë§Œë“¤ê³  ëª¨ë¸ì„ í‰ê°€í•˜ì—¬, ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì€ Nê°œì˜ featureë¥¼ ì„ ì •
  - Feature Selection
    - í•™ìŠµ ë°ì´í„°ì— ëœë¤ featureë¥¼ ì¶”ê°€, 1ì°¨ í•™ìŠµ ë•Œì˜ best modelë¡œ ì¬í•™ìŠµ
    - ê° featureì˜ ì¤‘ìš”ë„ë¥¼ ì¸¡ì •í•˜ì—¬ ì¤‘ìš”ë„ê°€ ë‚®ì€ featureë¥¼ ì œê±°
- 2ì°¨ í•™ìŠµ
  - Feature Engineeringì´ ëë‚œ featureë¡œ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹
  - ì•™ìƒë¸”, ìŠ¤íƒœí‚¹ ëª¨ë¸ ìƒì„±

### Compete

- ëŒ€íšŒ ì°¸ê°€ë¥¼ ìœ„í•œ í•™ìŠµ , ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ì§€ë§Œ ì„±ëŠ¥ì´ ì¢‹ë‹¤.
- ë°ì´í„°ì…‹ì˜ í¬ê¸°ì™€ total_time_limit íŒŒë¼ë¯¸í„°ì— ë”°ë¼ì„œ validation strategyê°€ ì•Œì•„ì„œ ì„ íƒ
  - 80:20 train/test split / 5-fold CV / 10-fold CV ì…‹ ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©
- Linear, Decision Tree, Random Forest, Extra Trees, LightGBM, Xgboost, CatBoost, Neural Network, Nearest Neighbors ëª¨ë¸ì„ ì‚¬ìš©
- ì•™ìƒë¸”ê³¼ ìŠ¤íƒœí‚¹ì„ ì‚¬ìš©í•œë‹¤.
- Reportì— learning curveë§Œ ì œê³µ

---

# âœ¨

- ğŸ“Œ ì˜ˆì¸¡ê°’ ìƒì„±, ğŸ“Œ í‰ê°€ ì§€í‘œ ê³„ì‚°

```python
from supervised.automl import AutoML

# AutoML ëª¨ë¸ ìƒì„± (GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •)
automl = AutoML(
    algorithms=["LightGBM", "CatBoost", "Xgboost", "Extra Trees"],  # âœ… XGBoost & ExtraTrees ì¶”ê°€
    mode="Compete",
    total_time_limit=3600,  # ì‹¤í–‰ ì‹œê°„ ì œí•œ (ì´ˆ ë‹¨ìœ„, 1ì‹œê°„)
    eval_metric="auc",  # âœ… ROC-AUC
    random_state=42,
    ml_task="binary_classification",  # ì´ì§„ ë¶„ë¥˜ ë¬¸ì œ
    train_ensemble=True,  # âœ… ì•™ìƒë¸” í•™ìŠµ ì‚¬ìš© ì—¬ë¶€
    optuna_time_budget=3600,  # âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œê°„
    n_jobs=-1  # âœ… CPU ì½”ì–´ë¥¼ ìµœëŒ€í•œ í™œìš©
)

automl.fit(X_train, y_train)

# ëª¨ë¸ ì˜ˆì¸¡
y_pred = automl.predict(X_test)

# í™•ë¥  ì˜ˆì¸¡ (ROC-AUC í‰ê°€ë¥¼ ìœ„í•´)
y_pred_proba = automl.predict_proba(X_test)[:, 1]  # 1 í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ ê°’
```

- ğŸ“Œ í‰ê°€ ì§€í‘œ ê³„ì‚°

```python
from sklearn.metrics import roc_auc_score, accuracy_score, classification_report

# AUC-ROC ì ìˆ˜
auc_score = roc_auc_score(y_test, y_pred_proba)
print(f"ğŸ¯ ROC-AUC Score: {auc_score:.4f}")

# ì •í™•ë„ (Accuracy)
accuracy = accuracy_score(y_test, y_pred)
print(f"âœ… Accuracy: {accuracy:.4f}")

# ìƒì„¸ ì„±ëŠ¥ ì§€í‘œ ì¶œë ¥
print(classification_report(y_test, y_pred))
```

- ğŸ“Œ í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™”

```python
import matplotlib.pyplot as plt

# ëª¨ë¸ë³„ í”¼ì²˜ ì¤‘ìš”ë„ í™•ì¸
importance_df = automl.report()  # AutoMLì´ ìƒì„±í•œ ë³´ê³ ì„œ

# ì¤‘ìš”ë„ ì¶œë ¥
importance_df = importance_df[["name", "importance"]].sort_values(by="importance", ascending=False)

# ì‹œê°í™”
plt.figure(figsize=(10, 6))
plt.barh(importance_df["name"], importance_df["importance"])
plt.xlabel("Feature Importance")
plt.ylabel("Features")
plt.title("Feature Importance of AutoML Model")
plt.gca().invert_yaxis()
plt.show()
```

- ğŸ“Œ ëª¨ë¸ ì €ì¥

```python
import joblib

# ëª¨ë¸ ì €ì¥
joblib.dump(automl, "automl_model.pkl")
print("âœ… ëª¨ë¸ì´ automl_model.pkl íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
```

- ğŸ“Œ ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ í›„ ì˜ˆì¸¡

```python
# ëª¨ë¸ ë¡œë“œ
loaded_model = joblib.load("automl_model.pkl")

# ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡
new_predictions = loaded_model.predict(X_test)
print(new_predictions)
```

---

# ğŸ”—ë ˆí¼ëŸ°ìŠ¤

## ì°¸ê³  ê°•ì˜/ê¸€

- [mljar-supervised ì‚¬ìš©ë²•](https://www.ai-bio.info/usage/mljar-supervised-usage)
