# 🚀 학습 키워드

## 정의

- Gradient Descent 방법 = Steepest Descent 방법
- 함수 값이 낮아지는 경향으로 독립 변수 값을 변형시켜가면서 최종적으로는 최소 함수 값을 갖도록 하는 독립 변수 값을 찾는 방법

## 키워드

> 앞이 보이지 않는 안개가 낀 산을 내려올 때는 모든 방향으로 산을 더듬어가며 산의 높이가 가장 낮아지는 방향으로 한 발씩 내딛어갈 수 있다

---

# 📝새로 배운 개념

## Gradient Descent의 목적과 사용이유

- 목적 : 함수의 최소값을 찾는 문제에서 활용
  - 미분 계수가 0인 지점을 찾는 방식이 아닌 gradient descent를 이용해 함수의 최소값을 찾는 주된이유
    1. 우리가 주로 실제 분석에서 다루는 함수들은 닫힌 형태(clsed form)가 아니거나 함수의 형태가 복잡해 (비선형 함수) 미분 계수와 그 근을 계산하기 어려운 경우가 많고,
    2. 실제 미분 계수를 계산하는 과정을 컴퓨터로 구현하는 것에 비해 경사하강법은 컴퓨터로 비교적 쉽게 구현할 수 있다.
    3. 데이터 양이 매우 큰 경우 gradient descent와 같은 반복(iterative)적인 방법을 통해 해를 구하면 계산량 측면에서 더 효율적으로 해를 구할 수 있다.

## Gradient Descent의 수식 유도

- 함수의 기울기 (즉, gradient)를 이용해 $x$의 값을 어디로 옮겼을 때 함수가 최소값을 찾는지 알아보는 방법
  - 기울기가 양수라는 것 : $x$값이 커질 수록 함수 값이 커진다
  - 기울기가 음수라는 것 : $x$값이 작아질 수록 함수 값이 작아진다.
  - 기울기의 값이 크다 (= 가파르다), $x$의 위치가 최소값 / 최댓값에 해당되는 $x$ 좌표로부터 멀리 떨어져있는 것을 의미하기도 함
- 방향 성분 이용
  - 특정 포인트 $x$에서 $x$가 커질 수록 함수값이 커지는 중 (기울기의 부호는 양수) -> 음의 방향으로 $x$를 옮겨야 할 것
  - 특정 포인트 $x$에서 $x$가 커질 수록 함수값이 작아지는 중 (기울기의 부호는 음수) -> 양의 방향으로 $x$를 옮겨야 할 것
  - $x\_{i+1} = x_i - 이동거리 * 기울기의 부호$
- 크기 이용

  - `이동거리` 라는 factor를 어떻게 구할지 생각해보자.
  - 미분계수(즉, 기울기 혹은 gradient)값은 극소값에 가까울 수록 그 값이 작아진다.
    - 사실 극대값에 가까울 때에도 미분 계수는 작아짐,
    - gradient descent과정에서 극대값에 머물러 있는 경우, 극히 드물다, 고려X
  - 따라서, 이동거리에 사용할 값 : gradient의 크기와 비례하는 factor를 이용하면 현재 $x$의 값이 극소값에서 멀 때는 많이 이동하고, 극소값에 가까워졌을 때는 조금씩 이동
  - 즉, 이동거리 : gradient값을 직접 이용
    - 이동거리를 적절히 사용자가 조절할 수 있게 수식을 조정해줌으로써 상황에 맞게 이동거리를 맞춰나간다.

- 최종 수식
  $$x_{i+1} = x_i - \alpha \frac{df}{dx}(x_i)$$
- 다변수 함수에 대해 확장
  $$x_{i+1} = x_i - \alpha \nabla f(x_i)$$

### 적절한 크기의 Step Size

- 너무 크게 -> 최소값을 계산하도록 수렴하지 못하고 함수 값이 계속 커지는 방향으로 최적화 진행
- 너무 작게 -> 발산하지는 않을 수 있지만 최적의 $x$를 구하는데 소요되는 시간이 오래 걸린다는 단점

---

## ✨ Local Minima 문제

- gradient descent 알고리즘을 시작하는 위치는 매번 랜덤하기 때문에 어떤 경우에는 Local Minima에 빠져 계속 헤어나오지 못하는 경우도 생김

---

# 🔗레퍼런스

## 참고 강의/글

- [공돌이의 수학정리노트](https://angeloyeo.github.io/2020/08/16/gradient_descent.html)
