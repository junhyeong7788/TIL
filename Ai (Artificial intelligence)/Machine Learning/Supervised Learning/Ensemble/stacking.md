# 🚀 학습 키워드

## 정의

Stacking : 앙상블 학습 기법 중 하나로, 여러 개의 서로 다른 기본 모델(Base Models)을 학습시키고, 그 결과를 새로운 메타 모델(Meta Model)이 학습하여 최종 예측을 수행하는 방식

## 키워드

## 다양한 모델을 조합하여 더 강력한 성능을 내는 방법 <-> Bagging, Boosting 과는 다른 구조

# 📝 Stacking의 핵심 개념

### 다양한 모델 활용 (Base Models)

- 여러 개의 머신러닝 모델을 사용하여 각각 예측을 수행
- 모델 간의 상호보완적인 특성을 활용하여 성능을 향상

### 메타 모델 (Meta Model)

- Base Model들이 만든 **예측값을 입력** 으로 받아 최종 예측을 수행하는 모델
  - 일반적으로 선형회귀, 로지스틱 회귀, 랜덤 포레스트 등이 사용

### 2단계 학습 구조

- 1단계 : 여러 개의 기본 모델(Base Model) 개별 학습 후 예측값 생성
- 2단계 : 메타 모델이 기본 모델들의 예측값을 입력으로 받아 최종 예측 수행

## Stacking의 장단점

- 장점
  - 서로 다른 알고리즘을 결합하여 높은 예측 성능을 기대할 수 있음
  - 특정 모델이 잘못된 예측을 하더라도 다른 모델이 이를 보완가능
  - 여러 모델의 조합을 통해 복잡한 문제에 효과적
- 단점
  - 학습 속도가 느림
  - 과적합 가능성이 높음
  - 해석이 어렵고, 하이퍼파라미터 튜닝이 어려움

---

# ✨

## Stacking vs Bagging vs Boosting

|      ---       |            Stacking            |              Bagging              |                         Boosting                         |
| :------------: | :----------------------------: | :-------------------------------: | :------------------------------------------------------: |
|   학습 방식    | 다른 모델들의 예측 결과를 활용 |   여러 개의 모델을 병렬로 학습    |             이전 모델의 오류를 보완하며 학습             |
|      목표      |   모델 조합을 통해 성능 향상   |     과적합 방지 및 분산 감소      |                  오류 수정 및 성능 개선                  |
| 대표 알고리즘  |     Stacked Generalization     | Random Forest , BaggingClassifier | AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost |
| 오류 처리 방식 |   다양한 모델의 강점을 활용    |  데이터 샘플링을 통한 분산 감소   |              오분류된 데이터에 가중치 부여               |

---

# 💻활용 사례

## 활발한 사용 분야

- ✔ 금융(Finance): 주가 예측, 신용 평가 모델
- ✔ 의료(Medical Diagnosis): 질병 진단, 의료 영상 분석
- ✔ 자연어 처리(NLP): 감성 분석, 문서 분류
- ✔ 컴퓨터 비전(CV): 이미지 분류, 객체 탐지
- ✔ 추천 시스템(Recommendation System): 사용자 맞춤형 추천

---

# 🔗레퍼런스

## 참고 강의/글

- [pythonML - 스태킹 앙상블(Stacking ensemble)이란?](https://resultofeffort.tistory.com/37)
