> 울산대학교 Ai융합전공, 2024년 4학년 1학기, Ai 정보이론 / 수업 내용 및 추가 학습 서술

# 함수
: 함수 f는 다음과 같은 튜플 (X, Y, graph f)이다.

- X 는 집합이며, 이를 f의 정의역이라고 한다.
- Y는 집합이며, 이를 f의 공역이라고 한다.
- graph f는 곱집합 X * Y 의 부분 집합이며, 이를 f의 그래프라고 한다.

- 함수는 정의역의 각 원소를 정확히 하나의 공역 원소에 대응 시킨다.
- f : X -> Y 는 f가 정의역 X, 공역 Y를 갖는 함수라는 뜻 ( f(x = y)와 같은 뜻)
- f(X)는 치역이라고 부른다.

# 엔트로피, 전송 정보

### 정의 - A Measure of Uncertainty

The entropy H(X) of a discrete random variable X is defined by

![equation](<https://latex.codecogs.com/svg.image?\huge%20H(X)=-\sum_{x\epsilon\chi}^{}p(x)logp(x).=\sum%20p(x)log\frac{1}{p(x)}>)

- log 밑이 2이므로 엔트로피는 비트로 표시된다.
- 엔트로피 H(X)는 랜덤변수 log( 1 / p(X) ) 의 기댓값, 즉 아래 식으로 간주할 수 있다.

  ![equation](<https://latex.codecogs.com/svg.image?\huge&space;&space;H(X)=E_{p}log\frac{1}{p(X)}>)

- H(X)는 0보다 크거나 같다.
- 최적의 전략 하에서 그 사건을 예측하는 데에 필요한 질문 개수
	- 최적의 전략 하에서 필요한 질문 개수에 대한 기댓값

# 확률 변수 ( Random variable)

: 사건으로 인해 그 값이 확률적으로 정해지는 변수를 의미

- 확률을 수식적으로 표현하기 위해 등장한 개념
- 확률 : 어떠한 사건이 일어날 가능성을 수로 표현한 것

# 이산 확률 변수 ( Discrete random variable )

: 확률변수 X가 유한개의 값을 가질 때 그 확률 변수

확률변수는 유한하거나, 셀 수 있는 무한한 개별로 나타낼 수 있으면 **이산변수**라고 한다.

```
ex ) 확률 변수가 0과 1같이 셀수 있는 형태이거나 유한한 형태라면 이산확률변수
```

- 이산 확률을 표현하는 이산확률모델은 표본공간의 결과에 따라 0과 1사이의 값을 할당한다.
- 이산랜덤변수 X가 있을 때 그 확률모델을 확률질량함수 (PMF : probability mass finctnion)이라고 한다.
- 유한개의 값(Finite), 혹은 자연수의 부분집합과 일대일 대응이 가능한 (Countable) 값으로 구성.

# 의미

: 확률이 낮을수록, 어떤 정보일지는 불확실하게 되고, 우리는 이때 '정보가 많다', '엔트로피가 높다'고 표현한다.
: 확률이 높을수록, 어떤 정보일지는 확실하게 되고, 우리는 이때 '정보가 적다', '엔트로피가 낮다'고 표현한다.

기본 : 어떤 사람이 정보를 더 많이 알수록 새롭게 알 수 있는 정보는 적어진다는 것

```ad-example
어떤 사건의 확률이 매우 높다.
우리는 그 사건이 발생해도 별로 놀라지 않다.
즉 이 사건은 적은 정보를 제공한다.
반대로, 만약 사건이 불확실하다면, 그 사건이 일어났을 때 훨씬 유용한 정보를 제공한다.
-> 정보량은 확률에 반비례한다.

```

EPL, PL, 분데스리가 등 축구리그가 있다.
EPL이 인기가 많은 이유는 1위팀과 하위 팀의 전력차이가 발생하지만 누가 이길지 예측하기가 힘들다. `불확실성하다 = 예측이 불가하다.` / 엔트로피가 높다.

하지만 다른 축구 리그들은 1위팀과 하위팀의 전력 차이가 월등히 나기 때문에 승부예측하기가 쉽다. `불확실성이 없어진다.` / 엔트로피가 낮다.

# 예시

```
Simple Space = {East, West} # 해가 동쪽에서 뜨나 서쪽에서 뜨나
East -> 0, West -> 1
```

![alt text](<Information Theory Attached file/Pasted image 20240304192200.png>)

-> 0에 근사하므로 불확실성이 없어진다. 엔트로피가 0이기때문에 시행결과는 아무런 정보도 전달하지 않는다.

```
fair coin
앞 -> 0 , 뒤 -> 1
P(0) = 1/2, P(1) = 1/2 => H(X)
```

![alt text](<Information Theory Attached file/Pasted image 20240304191532.png>)

- 동전던지기를 시행했을 때 결과값의 엔트로피는 공정한 동전일 때 가장 높게 나온다. ( 앞, 뒷면이 나올 확률이 각각 1/2로 같을 경우)
- 이런 경우가 불확실성을 가장 극대화 시키고 결과값을 예상하기가 가장 어렵다는 것을 의미
- 결과값은 1비트에 해당하는 정보를 가지게 된다.
- But, 만약 우리가 이 동전이 공정하지 않다면, 앞면 p/ 뒷면 q로 이미 알고 있다면 불확실성은 더 떨어질 것이다. (이는 동전을 던질 때마다 특정한 면이 나올 확률이 더 높기 때문)

# 개념의 일반화

```
Binary Random Variable의 예시
Let X = { 1, with probability p
		{ 0, with probability 1-p
```

- Then,

  ![equation](<https://latex.codecogs.com/svg.image?\huge&space;H(X)=-plogp-(1-p)log(1-p)>)
- 이는 0과 1사이의 p에 대한 함수로, p가 0이나 1에 가까워질수록 엔트로피는 0에 가까워진다.

![alt text](<Information Theory Attached file/스크린샷 2024-03-04 19.34.34.png>)


# 참조
- [[1. 확률 개요 (딥러닝을 위한 통계) - 24.02.25]]
- [[2. 확률 변수와 확률 분포 (딥러닝을 위한 통계) - 24.02.25]]
- [[3. 이산확률분포 (딥러닝을 위한 통계)]]