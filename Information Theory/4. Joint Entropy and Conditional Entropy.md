> 울산대학교 Ai융합전공, 2024년 4학년 1학기, Ai 정보이론 / 수업 내용 및 추가 학습 서술

- 엔트로피 역시 확률 사용 그러므로 결합 엔트로피와 조건부 엔트로피가 존재한다.
- 엔트로피는 평균 정보량
  - 정보량 : 어떤 사건이 가지고 있는 '놀람의 정도'
  - 일어날 확률이 적은 사건이 일어났을 때 사람들은 놀란다.
  - 따라서 드물게 일어나는 일 일수록 정보량이 크다.
- 엔트로피는 발생가능한 사건들의 정보량의 평균(기댓값)을 낸 것
  - 따라서 평균 놀람의 정도 또는 불확실성으로 생각가능
  - 여러 사건들이 발생할 확률이 비슷할수록 불확실성, 즉 엔트로피는 커진다.
  - 결과를 예측하기 쉬울수록 엔트로피는 작다.

# Joint Entropy

: 두 개 이상의 확률변수들의 결합확률의 엔트로피

확률변수 X, Y의 결합엔트로피는 H(X, Y)로 나타난다.

![equation](<https://latex.codecogs.com/svg.image?\huge&space;H(X,Y)=-\sum_{x\epsilon\chi}^{}\sum_{y\epsilon&space;Y}^{}p(x,y)logp(x,y)>)

![equation](<https://latex.codecogs.com/svg.image?\huge&space;=-E[logp(X,Y)]>)

ex) 결합확률분포표와 계산
![alt text](<Information Theory Attached file/Pasted image 20240313184936.png>)

# Conditional Entropy

: X가 주어졌을 때 Y의 엔트로피는 H(Y|X)로 표현, Y가 주어졌을 때 X의 엔트로피는 H(X|Y)로 표현

![equation](<https://latex.codecogs.com/svg.image?\huge&space;H(Y|X)=\sum_{x\epsilon\chi}^{}p(x)H(Y|X=x)=-\sum_{x\epsilon\chi}^{}p(x)\sum_{y\varepsilon&space;Y}^{}p(y|x)logp(y|x)=-\sum_{x\epsilon\chi}^{}\sum_{y\epsilon&space;Y}^{}p(x,y)logp(y|x)=-E[logp(Y|X)]>)

- 여기서 p*{X}(x)와 p*{Y|X}(y|x)와 같은 것들은 주변확률 질량함수로 나타낸다.

### 주변 확률 질량 함수 ( Marginal Probability Mass Function )

- 두 확률 변수 중에서 하나의 확률변수에 대해서만 확률분포를 나타낸 함수
  ![equation](<https://latex.codecogs.com/svg.image?\huge&space;P_{X}(X)=\sum{y_{i}}P_{XY}(x,y_{i})>)
  ![equation](<https://latex.codecogs.com/svg.image?\huge&space;P_{Y}(Y)=\sum{x_{i}}P_{XY}(x_{i},y)>)

### 조건부 확률 질량 함수
: 특정한 조건이 존재하는 상황에서 어떠한 사건이 발생할 확률

- 조건부 확률 질량 함수 공식
![equation](https://latex.codecogs.com/svg.image?\huge&space;P_{Y|X}(y|x)=\frac{P_{XY}(x,y)}{P_{X}(x)})

- 두개가 동시에 발생할 확률 / x의 주변확률 값

ex) 결합확률분포표와 주변확률, 계산
![alt text](<Information Theory Attached file/Pasted image 20240313190804.png>)
