
# Communication System
1. 메시지 (W) : 우리가 전송하려고 하는 정보 (이메일, 텍스트 메시지 등)
2. 인코더 (Encoder)  : 메시지를 코드화하는 장치
	- 이는 메시지를 전송하기 적합한 형식 ( X^n ) 으로 변환
	- 텍스트를 이진코드(0과 1의 시퀀스)로 변환하는 것
3. 채널 (Channel) : 메시지가 전송되는 경로
	- 여기에는 잡음이나 간섭이 존재가능, 이 채널은 p(y|x)라는 확률 전이 행렬로 표현되는데, 이는 입력 심볼 x가 주어졌을 때 출력 심볼 y가 나타날 확률을 나타냄
4. 디코더 (Decoder) : 수진된 메시지 (Y^n)를 원래의 메시지 형식(^W)으로 변환하는 장치
	- 디코더는 채널을 통해 전달된 신호를 해석하여 원래의 메시지를 추정
5. 메시지의 추정치(^W): 디코더가 복원한 메시지
	- 이는 원래 메시지 W와 최대한 일치하도록 노력하지만, 채널 잡음 등으로 인해 완전히 동일하지 않을 수도 있다.

![[Pasted image 20240611140820.png]]
### 정의
- 그림에서 정의한 이산 채널은 입력 알파벳 X와 출력 알파벳 Y, 그리고 확률 전이 행렬 p(y|x)로 구성된 시스템
	- 여기서 p(y|x)는 우리가 심볼 x를 보냈을 때 출력 심볼 y를 관찰할 확률을 나타냄
	- 쉽게 말해, 통신 시스템은 정보를 전송하고 수신하는 과정에서 발생하는 전달과정을 나타냄
	- 인코더와 디코더는 정보의 변환을 담당하고, 채널은 정보를 전송하는 경로를 제공
	- **이 과정에서 정보가 변형될 확률을 확률 전이 행렬로 모델링한다**

#### 확률전이행렬(Probability Transition Matrix)
- 이산 시간 마코프체인 (Discrete-time Markov chain)에서 한 상태에서 다른 상태로 이동할 확률을 나타내는 행렬
	- 이는 시스템이 시간에 따라 어떻게 변화하는지를 모델링하는 데 사용

**정의**
- 확률 전이 행렬 P는 다음과 같은 n * n행렬이다.
- 여기서 n은 상태의 개수이다.
![[Pasted image 20240612132129.png]]
- 각 P_ij는 상태 i에서 상태 j로 전이될 확률을 나타내며, 다음 조건을 만족
	- Pij >= 0 (모든 전이 확률은 0 이상)
	- 각 행의 합이 1 : ![[Pasted image 20240612132229.png]] (각 상태에서 다른 상태로 전이될 확률의 합이 1)

**예시**
- 3개의 상태 (A,B,C)가 있는 마코프 체인의 확률 전이 행렬은 다음과 같이 나타낼 수 있다.
- ![[Pasted image 20240612132335.png]]
- p_11​=0.5: 상태 A에서 상태 A로 전이될 확률은 0.5
- p_12​=0.3: 상태 A에서 상태 B로 전이될 확률은 0.3
- p_13​=0.2: 상태 A에서 상태 C로 전이될 확률은 0.2
- p_21​=0.1: 상태 B에서 상태 A로 전이될 확률은 0.1
- p_22​=0.6: 상태 B에서 상태 B로 전이될 확률은 0.6
- p_23​=0.3: 상태 B에서 상태 C로 전이될 확률은 0.3
- p_31​=0.4: 상태 C에서 상태 A로 전이될 확률은 0.4
- p_32​=0.4: 상태 C에서 상태 B로 전이될 확률은 0.4
- p_33​=0.2: 상태 C에서 상태 C로 전이될 확률은 0.2

-> 각 행의 합이 1이며, 행렬의 각 요소는 상태 전이 확률을 나타낸다,
이는 시스템의 동적 변화를 수학적으로 표현하고 분석하는 데 중요한 도구.

# Channel Capacity
- 채널 용량은 "정보" 채널 용량과 "운영"채널 용량으로 나눌 수 있다.

1. 정보 채널 용량 (Information Channel Capacity)
- 이산 기억 없음 채널의 정보 채널 용량은 다음과 같이 정의
![[Pasted image 20240611141122.png]]
- I(X;Y) : 입력 신호 X와 출력 신호 Y 사이의 상호 정보(mutual information)를 의미
- max_{p(x)} : 입력 신호 X의 확률분포를 최적화하여 상호 정보를 최대화하는 것을 나타냄

2. 운영 채널 용량(Operational Channel Capacity)
- 운영 채널 용량은 정보를 임의의 낮은 오류 확률로 전송할 수 있는 채널 사용당 비트 수의 최고 속도로 정의

### 정리
- 정보채널용량 : 주어진 채널에서 입력과 출력 간의 정보 전달 효율성을 최대화하는 최적의 입력 신호 분포를 찾는 과정
	- 이는 채널이 허용할 수 있는 최대 데이터 전송속도를 의미
- 운영채널용량 : 실제로 통신 시스템이 오류 없이 데이터를 얼마나 빠르게 전송할 수 있는 지를 나타냄
	- 이는 실질적인 데이터 전송 능력을 의미

- 채널용량은 통신 채널이 얼마나 많은 정보를 안정적으로 전달 할 수 있는지를 나타내는 중요한 개념
	- 이를 통해 채널의 성능을 평가하고, 데이터 전송의 효율성을 극대화 할 수 있다.
#### 이산기억없음 채널
- 이산적 입력과 출력 : 채널의 입력과 출력은 이산적인 기호들로 구성, 
	- 즉 특정한 유한한 집합의 기호들만 사용
- 기억 없음 : 현재 입력에 대한 출력은 오직 현재 입력과 관련이 있으며, 이전의 입력이나 출력과는 무관하다. 
	- 이는 각 전송이 독립적으로 이루어진다는 것을 의미
- 확률적 전송 : 입력 기호가 주어졌을 때 출력 기호는 특정 확률로 결정
	- 이 확률은 채널의 특성으로 정의

- 대표적인 예시 : 이진 대칭 채널 (BSC)
# **Examples of Channel Capacity**

# Noiseless Binary Channel (잡음이 없는 이진 채널)
![[Pasted image 20240611231020.png]]
- X : 입력 알파벳으로 0과 1의 두가지 값을 가질 수 있다
- Y : 출력 알파벳으로 0과 1의 두가지 값을 가질 수 있다
	- 이 채널은 잡음이 없기 때문에 입력 값이 그대로 출력 값으로 전달
	- 즉, 0을 입력하면 출력도 0이 되고 1을 입력하면 출력도 1이다.

- 운영용량 (Operational Capacity)
	- 이 채널의 운영용량은 1비트
	- 이는 이 채널을 통해서 1비트의 정보를 손실 없이 전송할 수 있다는 것을 의미
- 정보 용량 (Information Capacity)
	- ![[Pasted image 20240611230819.png]]
	- 최적의 입력 분포 p(x)는 p(x)=(0.5,0.5)로, 이는 **0과 1이 각각 50%의 확률**로 발생함을 의미합니다.
	- 이 조건에서 상호 정보 I(X; Y)는 최대 1비트가 됩니다. 이는 입력과 출력이 **완전히 일치**하기 때문에 가능한 최대 정보량입니다.

### 정리
- 잡음이 없는 이진 채널 : 입력 값이 그대로 출력 값으로 전달되므로, 입력과 출력 사이의 정보전달 효율성이 최대가 됨
	- 이를 통해 이 채널의 용량은 1비트로 정의되며, 이는 정보 손실 없이 전송할 수 있는 최대 비트 수를 의미

# Noisy Binary Channel with Nonoverlapping Outputs (잡음이 있는 이진  채널과 비중첩 출력)
![[Pasted image 20240611231621.png]]
- X : 입력알파벳 : 0과 1의 두가지 값
- Y : 출력 알파벳 : 1,2,3,4의 네 가지 값
	- 이 채널에서 입력 0은 출력 1과 2 중 하나로 전달 될 수 있고, 입력 1은 출력 3과 4 중 하나로 전달가능
	- 각 출력은 일정한 확률로 결정

- 전이 확률 (Transition Probabilities)
	- 입력 X=0은 출력 Y=1 또는 Y=2로 변환될 수 있다. 각 확률은 1/2 이다.
	- 입력 X=1은 출력 Y=3 또는 Y=4로 변환될 수 있다. 각 확률은 1/3과 2/3 이다.

- 운영 용량 (Operational Capacity)
	- 이 채널의 운영 용량은 1비트
	- 이는 이 채널을 통해 1비트의 정보를 손실 없이 전송할 수 있다는 것을 의미

### 정리
- 이 채널은 출력 값이 입력 값에 따라 다르게 결정, 출력이 겹치지 않는다는 특징
- 입력 신호가 0일 때와 1일 때 출력되는 값의 범위가 다르기 때문에 이는 출력 신호를 통해 입력 신호를 구분할 수 있게 함
	- 이로 인해 상호 정보는 최대 1비트가 되며, 이는 정보용량을 결정함

# Noisy Tyoerwriter (잡음이 있는 타자기) 모델
![[Pasted image 20240611232056.png]]
- **입력 알파벳 (Input Alphabet)**: A, B, C, ..., Z (총 26개 문자)
- **출력 알파벳 (Output Alphabet)**: A, B, C, ..., Z (총 26개 문자)
	- 입력 문자와 출력 문자 사이에 잡음이 있어 정확히 대응되지 않는다.
	- 예를 들어, 입력이 A일 때 출력이 A, B, C 중 하나가 될 수 있다.

- **노이즈 채널 (Noisy Channel)**: 입력 신호가 잡음을 통해 출력될 때, 입력과 출력 사이에 오차가 발생할 수 있다
- **노이즈 없는 부분 집합 (Noiseless Subset of Inputs)**: 잡음 없는 입력의 하위 집합을 사용하여 에러 없이 통신할 수 있다.

- 그림에서는 입력과 출력 사이의 대응을 시각적으로 보여줌
	- 예를 들어, 입력 A는 출력 A,B,C와 대응 될 수 있다.
	- 이는 다른 입력 문자들에도 적용
- 노이즈 없는 입력문자 사용
	- 모든 입력 문자를 사용하는 대신, 특정한 간격으로 입력 문자를 선택하여 사용하면 에러 없이 통신할 수 있다.
	- 예를 들어, 26개의 문자 중 절반인 13개 문자를 사용하여 통신 할 수 있다.

- 정보용량
	- ![[Pasted image 20240611232354.png]]
	- 이를 통해, 매 전송마다 13개의 기호 중 하나를 에러 없이 전송 할 수 있다는 것을 의미

### 정리
1. 입력과 불력의 불일치
2. 특정 입력 문자만 사용 : 모든 문자를 사용하면 에러 발생, 특정 문자를 사용하면 에러 없이 전송 가능
3. 정보 용량 : 특정 문자를 사용하여 에러 없이 최대 13개의 기호를 전송 가능

# Binary Symmetric Channel (1), 이진 대칭 채널
- 입력 알파벳 : 0과 1
- 출력 알파벳 : 0과 1

- 오류 확률 (Error Probability)
	- 입력 0이 출력 0으로 전달될 확률은 1−p
    - 입력 0이 출력 1로 잘못 전달될 확률은 p
    - 입력 1이 출력 1로 전달될 확률은 1−p
    - 입력 1이 출력 0으로 잘못 전달될 확률은 p

- 대칭성 (Symmetry)
	- 오류 확률 p는 입력 0과 1 모두에 대해 동일

![[Pasted image 20240611232929.png]]
- **입력 0에서 출력**:
    - 1−p의 확률로 0이 출력됩니다.
    - p의 확률로 1이 출력됩니다.
- **입력 1에서 출력**:
    - 1−p의 확률로 1이 출력됩니다.
    - p의 확률로 0이 출력됩니다.

### 정리
1. BSC의 단순함 : 오류가 있는 채널의 가장 단순한 모델, 입력 비트가 출력 비트로 전달 될 때 일정확률로 오류가 발생
2. 모든 수신 비트의 신뢰성 문제
	- 오류 확률 p로 인해 신뢰할 수 없다
	- 따라서 각 비트가 원래의 비트와 다를 가능성이 있다.
3. 정보 전송 가능성 : 이러한 채널을 사용하여 비트오류확률을 임의로 작게 만들면서도 정보 전송이 가능한 방법이 제시
	- 채널 코딩 이론을 통해 가능

# Binary Symmetric Channel (2), 이진 대칭 채널

- 상호 정보 계산
![[Pasted image 20240611233812.png]]

1. I(X; Y) = H(Y) - H(Y|X)
2. 조건부 엔트로피 H(Y∣X)는 입력 X에 따른 출력 Y의 엔트로피의 합으로 표현
   ![[Pasted image 20240611233955.png]]
3. 이진 대칭 채널에서는 H(Y∣X=x)가 일정한 값 H(p)로 나타납니다: 
   ![[Pasted image 20240611234002.png]]
4. 따라서, I(X;Y)=H(Y)−H(p)

- 정보 용량
	- 이제 상호정보를 최대화하기 위해 H(Y)의 최대값을 고려
	- 이진 대칭 채널의 최대 엔트로피 H(Y)는 1이다.
	- ![[Pasted image 20240611234040.png]]
	- 여기서 H(p)는 이진 확률 분포의 엔트로피
	- ![[Pasted image 20240611234059.png]]
	- 최종적으로 정보용량 C는 상호 정보의 최대값으로 정의,
	- ![[Pasted image 20240611234115.png]]

- 즉, 이진 대칭 채널의 정보용량은 1-H(p)비트이며, 이는 채널에서 정보를 오류 없이 전송 할 수 있는 최대 속도를 의미
- H(p)는 오류 확률 p에 따라 달라지며, p가 0이거나 1일 때 최소가 되고, p=0.5일 때 최대가 됨

# Binary Erasure Channel (1), 이진 소거 채널
![[Pasted image 20240611234552.png]]
- **입력 알파벳 (Input Alphabet)**: 0과 1의 두 가지 값을 가질 수 있다.
- **출력 알파벳 (Output Alphabet)**: 0, 1, e (e는 소거를 의미) 세 가지 값을 가질 수 있다.
- **소거 확률 (α)**: 입력 비트가 출력에서 사라질 확률을 나타낸다.
    - 입력 0이 1−α 확률로 출력 0으로 전달되고, α 확률로 소거된다.
    - 입력 1이 1−α 확률로 출력 1으로 전달되고, α 확률로 소거된다.

![[Pasted image 20240612022105.png]]

1. ![[Pasted image 20240612022056.png]]
2. 조건부엔트로피는 소거확률 α와 관계없이 일정, 즉 출력이 소거되지 않은 경우에는 H(Y|X)가 0이고, 소거된 경우에는 H(α)가 된다.
- ![[Pasted image 20240612022157.png]]
- 여기서 H(e) = 0이므로, ![[Pasted image 20240612022216.png]]
	- 엔트로피 H(e)는 소거 (e)가 되는 사건의 엔트로피를 의미
	- 소거가 발생하면 출력이 e로 고정
	- 즉, 출력이 e로 고정된 경우, 불확실성이 전혀 없다
	- 엔트로피는 불확실성을 측정하는 값이므로, 불확실성이 없는 경우 엔트로피는 0이 된다.
3.  출력 Y의 엔트로피 H(Y)는 다음과 같이 계산
- ![[Pasted image 20240612022246.png]]
- 여기서 H(α)는 소거확률 α의 엔트로피이고, π는 X=1일 확률이다.

- 최종적으로 정보용량 C는 상호 정보의 최대값으로 정의
- ![[Pasted image 20240612022538.png]]

### 정리
- 이진 소거 채널의 정보용량은 1-α비트이며, 이는 채널에서 소거 확률 α가 주어질 때 정보를 오류 없이 전송 할 수 있는 최대 속도를 의미

# Binary Erasure Channel (2), 이진 소거 채널

### 정보 용량 계산 과정
![[Pasted image 20240612023352.png]]
- 사건 E를 Y = e로 두면 다음과 같다.
- 그런 다음, P( X = 1 ) = π 로 두면, 다음과 같다.
#### 1. 출력 Y의 엔트로피 H(Y) 계산
- 우선, 출력 Y의 엔트로피 H(Y))를 구합니다. 여기서 E는 출력이 소거(e)가 되는 사건을 의미합니다.
![[Pasted image 20240612022738.png]]
- H(E)는 소거 확률 α의 엔트로피
- H(Y∣E)는 출력 Y가 소거되지 않을 때의 조건부 엔트로피
#### 2. H(Y)를 π로 표현
- 입력 X=1일 확률을 π라고 할 때, H(Y)는 다음과 같이 표현됩니다:
	- 위 조건을 도입하는 이유는 출력 Y의 엔트로피를 입력 확률과 소거 확률을 모두 고려하여 정확하게 계산하기 위해서이다. 
![[Pasted image 20240612022821.png]]
- H(α)는 출력이 소거될 확률(α)의 엔트로피
- (1−α)는 출력이 소거되지 않을 확률
- H(π)는 입력 X가 주어진 경우의 엔트로피
	- 이는 입력 X가 1일 확률와 관련된 불확실성(π)을 나타냄
#### 3. 상호 정보 I(X;Y) 계산
- 상호 정보 I(X;Y)는 다음과 같이 계산됩니다:
![[Pasted image 20240612022900.png]]
- H(Y∣X)는 입력 X가 주어졌을 때 출력 Y의 조건부 엔트로피로, 이는 H(α)와 동일
#### 4. 정보 용량 C 계산

- 최종적으로 정보 용량 C는 상호 정보의 최대값으로 정의되며, 다음과 같이 계산됩니다:
![[Pasted image 20240612022941.png]]
- H(α)는 소거 확률의 엔트로피로 상수이므로 제거할 수 있다.
- 최적의 확률 분포 π를 고려하면, 정보 용량은 1−α로 결정된다.



