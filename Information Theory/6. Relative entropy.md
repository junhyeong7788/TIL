# KL divergence ( Relative entropy )

: 분포가 discreate할 때, KL divergence를 정의한다.

```
- 두 확률 분포 간의 정보량 차이를 나타내는 개념
- 각각의 확률 분포의 차이를 정보량 측면에서 수치화하는 것이다.

- 두 확률 분포의 차이를 '수치화'한다. 확률분포의 차이를 수치화한다면, 실제 확률분포에 가까운 분포와 그렇지 않은 분포의 차이를 알아낼 수 있다.
- 따라서 어떤 것이 더 예측을 잘하는 확률분포인지를 알 수 있다.
```

- 엔트로피는 임의의 변수를 설명하는데 필요한 정보의 평균적인 양으로 간주될 수 있다.
- 상대 엔트로피는 두 분포 사이의 거리의 척도이다.
- 확률변수의 분포 p를 안다면, 우리는 평균 길이 H(p)를 갖는 코드를 구성할 수 있다. 대신 분포 q에 대한 코드를 사용한다면, H(p)+D(p||q)비트가 필요하다.

### Definition

![equation](<https://latex.codecogs.com/svg.image?\huge&space;&space;D(p||q)=\sum_{x\epsilon\chi}^{}p(x)log\frac{p(x)}{q(x)}=E_{p}log\frac{p(X)}{q(X)}.>)

- 두 PMF p(x), q(x)사이의 상대엔트로피 또는 KL거리는 D(p||q)로 정의한다.

### Some properties of Relative Entropy

- 상대 엔트로피는 실제 분포가 p일때, 분포 q라고 가정하는 비효율성의 척도이다.
- 상대엔트로피는 항상 음수가 아니다.
- 상대엔트로피는 대칭적이지 않고 삼각형 부등식을 만족시키지 않기 때문에 진정한 **거리**가 아니다. -> KL Divergence가 두 확률 분포 사이의 거리를 나타냈다면 P와 Q의 위치가 바뀌어도 같은 값을 가져야 하지만, 전혀 그렇지 않다.
- ![equation](https://latex.codecogs.com/svg.image?\huge&space;0log\frac{0}{0}=0,0log\frac{0}{q}=0,plog\frac{p}{0}=\infty&space;) 라는 규칙을 사용
- 따라서 p(x)>0과 q(x)=0인 ![equation](https://latex.codecogs.com/svg.image?\huge&space;x\epsilon\chi&space;) 가 있으면 ![equation](<https://latex.codecogs.com/svg.image?\huge&space;D(q||p)=\infty&space;>) 이다.

- 0 이상이다.
- KL Divergence가 0이라는 것은, 두 확률 분포가 동치임을 나타냄
- 기준으로 두는 확률 분포에 따라 비대칭적인 값을 갖는다.
- **중요** : 두 확률 분포가 비슷할 수록 더 작은 값을 갖는다는 것.
  - 결국 확률 분포의 차이를 수치화하고 싶다는 아이디어에서 출발한 개념이므로, 수치화되어 나타난 값이 작을수록 더 비슷한 확률 분포임을 보여준다

### Summary

![alt text](<Information Theory Attached file/Pasted image 20240320021317.png>)

- 결국 크로스 엔트로피 H(p,q)는 원래 p의 엔트로피 H(p)의 값에 마지막 항의 값 만큼을 더해준 것임을 알 수 있다.
- 이때, 더하는 '무언가'가 정보량의 차이, 이 정보량 차이의 식은 KL divergence와 같다.
- 즉, 불확실한 분포를 통해 확실한 분포를 추정해내고자 할때, 두 분포의 크로스 엔트로피값에서 실제 엔트로피 값을 만들기 위해 더해 주어야하는 정보량 차이를 나타낸다.
- KL Divergence를 상대 엔트로피라고도 한다.
