# 정보이론
- 두 가지 질문
	- 정보란 무엇인가?
	- 정보의 양을 측정할 수 있는가?
- 정보이론의 두가지 주요문제
	- 데이터 압축
	- 신뢰할 수 있는 통신을 위한 최대 전송 속도

# Channel Coding
- 채널 코딩 : 채널 잡음에 대해 통신 시스템의 저항성을 증가시키는 방법
- 아이디어 : 원래의 시퀀스를 정확하게 재구성하기 위해 채널 인코더를 통해 중복 데이터를 추가한다.

#### 용어
- 정보 길이(information length, k) : 원래의 정보 비트의 수
- 중복 (Redundancy, n-k) : 추가된 중복 비트의 수
- 코드율 (Code rate, $k/n$) : 정보 비트 수와 전체 비트 수의 비율 

### 과정
1. 이산 기억 없음 소스 : 송신기에서 정보 비트 k가 생성
2. 채널 인코더 : 정보비트 k를 받아 n비트 블록으로 변환, 이 과정에서 중복 비트가 추가
3. 이산 기억 없음 채널 : 인코딩된 n비트 블록이 채널을 통해 전송, 이 과정에서 잡음이 발생 가능
4. 채널 디코더 : 수신기에서 n비트 블록을 받아 원래의 정보비트 k로 복원
5. 목적지 : 디코딩 된 정보비트가 목적지에 도착

![[Pasted image 20240612024200.png]]

### 정리
- 채널 코딩은 중복 데이터를 추가하여 통신 시스템의 잡음 저항성을 높이는 방법
- 정보비트 k를 받아 n비트 블록으로 인코딩하고, 이를 통해 전송된 정보를 정확하게 복원
- 코드율은 정보 비트와 전체 비트의 비율로, 이는 시스템의 효율성을 나타냄

# Shannon's Second Theorem
- 가정 ( Assumptions )
1. 소스의 평균 정보율 ($H(S)/T_s$)
	- 여기서 $H(S)$는 소스의 엔트로피, $T_s$는 단위 시간
	- 단위 시간 당 비트 수로 측정 (bits/sec)
2. 단위 시간당 채널 용량 ($C/T_c$)
	- 여기서 $C$는 채널 용량, $T_c$는 단위 시간
	- 단위 시간당 비트 수로 측정
3. 질문
	- 심볼 오류 확률을 임의의 작은 값 $ϵ > 0$보다 작게 만드는 채널 코딩 방식이 존재하는가?

- 채널 코딩 정리 (Channel Coding Theorem) (Shannon)
	- **조건 1**
	- ![[Pasted image 20240612024712.png]]
	- 소스의 정보율이 채널 용량 이하인 경우, 임의의 작은 오류 확률을 달성할 수 있는 코딩 방식이 존재
	- **조건 2**
	- ![[Pasted image 20240612024731.png]]
	- 소스의 정보율이 채널 용량을 초과하는 경우, 임의의 작은 오류 확률을 달성하는 것은 불가능하다.

- 평균 정보율 : 정보 소스가 시간당 생성하는 정보의 양
- 채널 용량 : 채널이 시간당 전송할 수 있는 최대 정보의 양
- 샤논의 정리
	- 만약 소스의 정보 생성 속도가 채널의 정보 전송 속도보다 작거나 같다면, 매우 작은 오류로 데이터를 전송할 수 있는 코딩 방식이 존재
	- 반면에, 소스의 정보 생성 속도가 채널의 정보 전송 속도보다 크다면, 매우 작은 확률로 데이터를 전송하는 것은 불가능

### 정리
- 샤논의 두 번째 정리 : 통신시스템에서 소스의 정보율과 채널 용량간의 관계를 규명
- 이를 통해 통신 시스템의 성능 한계를 이해하고, 효율적인 데이터 전송 방법 설계 가능

# Channel Coding Theorem in BSC
- 이진 대칭 채널 (Binary Symmetric Channel, BSC)에서 샤논의 두번째 정리에 대해 설명

### 가정
1. 동일한 확률을 가진 심볼
	- $p(x_0)=p(x_1​)=0.5$
	- 이 경우 소스의 엔트로피 $H(S)=1$ 비트/심볼
2. 소스 시퀀스의 전송 속도
	- 소스 시퀀스는 $1/T_S​bps$의 속도로 채널 인코더에 적용
1. 채널 인코더의 심볼 생성 주기
	- 채널 인코더는 $T_C$ 초마다 심볼을 생성
2. BSC의 사용주기
	- 채널 인코더는 $T_C$초 마다 BSC를 사용

### 결과
1. 코드율
	- 코드율은 ![[Pasted image 20240612031358.png]]이다.
2. 샤논의 두 번째 정리
	- 샤논의 두 번째 정리에 따르면, BSC에서의 코드율 $r$ 은 채널용량 C보다 작거나 같아야한다.
	- BSC의 채널 용량 C는 다음과 같이 주어진다.
	- ![[Pasted image 20240612031453.png]]
	- 여기서 $p$는 오류 확률을 의미

### 정리
- **동일한 확률의 심볼**: 소스의 심볼이 동일한 확률을 가지며, 소스의 엔트로피가 1 비트/심볼임.
- **코드율**: 채널 인코더가 심볼을 생성하고 BSC를 사용하는 주기를 바탕으로 코드율을 계산함.
- **샤논의 정리**: 코드율 r은 BSC의 채널 용량 C보다 작거나 같아야 하며, BSC의 채널 용량은 오류 확률에 따라 계산됨.

# Repetition Code (반복 코드)
- 간단하지만 강력한 오류 수정 기법으로, 각 비트를 여러 번 반복하여 전송함으로써 신뢰성을 높임

### 가정
1. 입력 심볼이 동일한 확률로 발생:
    - 모든 입력 심볼이 동일한 확률로 발생한다고 가정합니다.
2. BSC (Binary Symmetric Channel):
    - 오류 확률 $p = 10^{-2}$를 가진 BSC를 사용합니다.

### 반복 코드 (Repetition Code)
1. 반복 횟수:
    - 각 비트를 $2m+1$번 반복합니다. 여기서 m은 반복 횟수의 절반입니다.
2. 다수결 규칙 (Majority Rule):
    - 수신된 비트 중 가장 많이 나타난 값을 선택하여 원래 비트를 복원합니다.

### 오류 확률 (Error Probability, P_e)
- 반복 코드를 사용한 경우의 오류 확률은 다음과 같다.
- ![[Pasted image 20240612031815.png]]
- $n=2m+1$은 비트의 반복 횟수입니다.
- ![[Pasted image 20240612031842.png]]는 이항 계수로, n번 중 i번 성공할 확률을 나타냅니다.
- p는 오류 확률, 1−p는 오류가 발생하지 않을 확률입니다.

![[Pasted image 20240612031933.png]]

- **오류 확률 그래프**: y축은 평균 오류 확률 $P_e$, $x$축은 코드율 $r$를 나타냅니다.
- **반복 코드 곡선**: 반복 코드를 사용한 경우의 오류 확률을 나타냅니다. 
	- 반복 횟수가 증가할수록 오류 확률이 감소하지만, 코드율도 함께 감소합니다.
- **제한값 (Limiting Value)**: 매우 낮은 오류 확률 $ϵ=10^{−8}$ 을 달성하기 위해 필요한 코드율과 비교합니다.

### 정리
- 반복 코드 : 각 비트를 여러 번 반복하여 전송함으로써 오류를 수정하는 방법
- 다수결 규칙을 사용하여 수신된 비트 중 가장 많이 나타난 값을 선택하여 원래 비트를 복원
- 반복 횟수가 증가 할수록 오류 확률은 감소하지만, 코드율도 함께 감소
- 그래프는 반복 코드를 사용한 경우의 평균 오류 확률과 코드율 간의 관계를 나타냄
- 최적의 오류 확률을 달성하기 위해 필요한 코드율을 확인하고, 이를 채널 용량과 비교

