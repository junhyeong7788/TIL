> 울산대학교 Ai융합전공, 2024년 4학년 1학기, Ai 정보이론 / 수업 내용 및 추가 학습 서술

# Maximum Entropy Distribution (최대 엔트로피 분포)
: 적어도 지정된 확률 분포 클래스의 다른 모든 구성원의 엔트로피만큼 큰 엔트로피를 갖는다.
- 정보엔트로피는 시스템의 불확실성을 측정하는 정보이론의 중요한 개념
	- 엔트로피가 높을수록 시스템의 불확실성이 높고, 엔트로피가 낮을수록 시스템의 불확실성이 낮다.
- 최대 엔트로피 분포는 주어진 제약 조건 하에서 엔트로피가 최대인 확률분포이다.
	- 즉, 시스템에 대한 정보가 부족할 때, 가능한 모든 결과에 대해 가장 불확실한 상태를 나타내는 분포

- 최대 엔트로피 원칙에 따르면 분포가 특정 클래스(일반적으로 지정된 속성 또는 측정값으로 정의됨)에 속한다는 것 외에는 알려진 것이 없는 경우 엔트로피가 *가장 큰 분포를 정보가 가장 적은 분포*로 선택해야한다.
	- 1. 엔트로피를 최대화하면 분포에 포함된 사전 정보의 양이 최소화된다.
	- 2. 많은 물리적 시스템은 시간이 지남에 따라 최대 엔트로피 구성으로 이동하는 경향이 있다.

: 여러가지 조건 하에서 최대 엔트로피를 만족하는 분포들.
- 정규분포는 이항분포에서 시행횟수가 증가하면 할수록 *근사(거의같다)하는 분포*이고, 이것은 최대 엔트로피 관점에서 유도 될 수 있다.
- 연속이 아닌 이산분포에서 최대 엔트로피는 *평균값*이다.
- 정보이론에서 모든 사건이 평균적인 확률을 지닐 때, 가장 엔트로피가 *높게* 나타나는 것이 이에 해당.

### Theorem 2.6.4
: 정리 2.6.4 H(X) ≤ log |X|, 여기서 |X]는 X에 대해 균일한 분포를 갖는 X인 경우에만 동등한 X 범위의 원소 수를 나타낸다.
- 확률변수의 엔트로피가 확률변수가 취할 수 있는 가능한 모든 값의 로그에 의해 상한이 주어진다고 말한다.
![equation](<https://latex.codecogs.com/svg.image?\huge&space;H(X)\leq&space;log\left|\chi\right|>)

- X가 범위 X에 대해 *균일한 분포*를 갖는 경우에만 동일함
- 공식 : 이산 확률 변수 X의 엔트로피 H(X)와 X의 가능한 값의 개수 |X| 사이의 관계
	- H(X)는 log|X|보다 크거나 같을 수 없다. 엔트로피가 log|X|에 도달하는 경우, X는 균일분포(Uniform distribution)를 따른다.
	- 즉, 모든 가능한 값이 동일한 확률을 가지는 분포이다.
- 엔트로피가 최대가 되는 경우
	- 모든 가능한 값이 동일한 확률을 가지는 경우
	- 시스템에 대한 정보가 부족하여 어떤 값이 나올지 예측할 수 없는 경우

![alt text](<Information Theory Attached file/Pasted image 20240326221606.png>)
- KL-divergence의 비음수성을 사용하여 *균일분포가 최대 엔트로피 분포임*을 보여주는 증명이다.

1. X는 이산 확률 변수이고, u(x)는 X의 균일 분포, p(x)는 X의 임의의 확률 분포라고 정의합니다.
2. KL 발산 D(p||u)를 계산합니다. KL 발산은 두 확률 분포 p(x)와 q(x) 사이의 차이를 측정하는 지표입니다.
3. KL 발산 D(p||u)는 항상 0 이상임을 증명합니다. 이는 Jensen의 불평등을 사용하여 증명할 수 있습니다.
4. KL 발산 D(p||u) = 0인 경우 p(x) = u(x)임을 증명합니다.

- 정리
	- D(p||u) : 항상 0이상이며, D(p||u) = 0인 경우 p(x)=u(x)이다.
	- 즉, X의 엔트로피 H(X)를 최대화하는 확률 분포는 균일분포 u(x)이다.
	- 균일분포 : 모든 가능한 값이 동일한 확률을 가지는 분포
# Conditioned Entropy

### Theorem 2.6.5

![equation](<https://latex.codecogs.com/svg.image?\huge&space;H(X|Y)\leq&space;H(X)>)
- X와 Y가 독립적인 경우에만 동등하게 적용된다. (같다.)
- Y의 정보가 우리의 X에 대해 알고 있는 것을 향상 시킬 수 있음을 의미한다.
	- 만약 X와 Y가 독립적이라면, Y를 알아도 X에 대한 우리의 지식은 향상되지 않으므로, 이 경우 `H(X∣Y)=H(X)` 가 된다.

- 증명
  ![equation](<https://latex.codecogs.com/svg.image?\huge&space;0\leq&space;I(X;Y)=H(X)-H(X|Y).>)
- 직관적으로 이 정리는 다른 확률변수 Y를 알면 X의 불확정성만 줄일 수 있다는 것을 말한다.
- 이는 평균적으로만 성립한다는 점에 유의한다.
- 구체적으로 H(X|Y=y)는 H(X)보다 크거나 작을 수 있으나, 평균적으로는 H(X|Y) = \sum_y(P(y)) H(X|Y = y) ≤ H(X)일 수 있다.
- 예를 들어 법원 사건에서 구체적인 새로운 증거는 불확실성을 증가시킬 수 있지만, 평균적인 증거는 불확실성을 감소시킨다.

![alt text](<Information Theory Attached file/Pasted image 20240326222654.png>)

- 따라서 Y = 2를 관찰하면 X의 불확실성이 증가하고, Y = 1을 관찰하면 감소하지만 평균적으로 불확실성이 감소합니다.

# Independence Bound on Entropy ( 엔트로피의 독립성 )

### Theorem 2.6.4

![alt text](<Information Theory Attached file/Pasted image 20240326222941.png>)
- 결합엔트로피 보다 각각의 엔트로피를 모두 더한 것이 크거나 같다.
- joint entropy , 부등호 , 각각의 엔트로피를 모두 더한 것

- 증명 : By the chain rule for entropies.
  ![alt text](<Information Theory Attached file/Pasted image 20240326223016.png>)
- 여기서 부등식은 정리 2.6.5로부터 직접 이어진다. 
- 우리는 X가 X_{i-1},..., X_1와 독립적인 경우에만, 모든 i에 대해 (즉, X가 독립적인 경우에만) 동등성을 갖는다.
# 참고
- 이항분포 : https://bioinformaticsandme.tistory.com/236
- 최대엔트로피분포 : https://en.wikipedia.org/wiki/Log_sum_inequality