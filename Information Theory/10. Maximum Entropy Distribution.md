> 울산대학교 Ai융합전공, 2024년 4학년 1학기, Ai 정보이론 / 수업 내용 및 추가 학습 서술

# Maximum Entropy Distribution

: 여러가지 조건 하에서 최대 엔트로피를 만족하는 분포들.
정규분포는 이항분포에서 시행횟수가 증가하면 할수록 근사하는 분포이고, 이것은 최대 엔트로피 관점에서 유도 될 수 있다.
연속이 아닌 이산분포에서 최대 엔트로피는 평균값이다.
정보이론에서 모든 사건이 평균적인 확률을 지닐 때 가장 엔트로피가 높게 나타나는 것이 이에 해당.

- 최대 엔트로피 확률분포는 적어도 지정된 확률분포 클래스의 다른 모든 구성원의 엔트로피만큼 큰 엔트로피를 갖는다.
- 최대 엔트로피 원칙에 따르면 분포가 특정 클래스에 속한다는 것 외에는 알려진 것이 없는 경우 엔트로피가 가장 큰 분포를 정보가 가장 적은 분포로 선택해야한다.

1. 엔트로피를 최대화하면 분포에 포함된 사전 정보의 양이 최소화된다.
2. 많은 물리적 시스템은 시간이 지남에 따라 최대 엔트로피 구성으로 이동하는 경향이 있다.

### Theorem 2.6.4

![equation](<https://latex.codecogs.com/svg.image?\huge&space;H(X)\leq&space;log\left|\chi\right|>)

- X가 범위 X에 대해 균일한 분포를 갖는 경우에만 동일함

![alt text](<Information Theory Attached file/Pasted image 20240326221606.png>)

# Conditioned Entropy

### Theorem 2.6.5

![equation](<https://latex.codecogs.com/svg.image?\huge&space;H(X|Y)\leq&space;H(X)>)

- X와 Y가 독립적인 경우에만 동등하게 적용된다.

- 증명
  ![equation](<https://latex.codecogs.com/svg.image?\huge&space;0\leq&space;I(X;Y)=H(X)-H(X|Y).>)

![alt text](<Information Theory Attached file/Pasted image 20240326222654.png>)

- 따라서 Y = 2를 관찰하면 X의 불확실성이 증가하고 Y = 1을 관찰하면 감소하지만 평균적으로 불확실성이 감소합니다.

# Independence Bound on Entropy ( 엔트로피의 독립성 )

### Theorem 2.6.4

![alt text](<Information Theory Attached file/Pasted image 20240326222941.png>)

- joint entropy , 부등호 , 각각의 엔트로피를 모두 더한 것

- 증명
- By the chain rule for entropies.
  ![alt text](<Information Theory Attached file/Pasted image 20240326223016.png>)
