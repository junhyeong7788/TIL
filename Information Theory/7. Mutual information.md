> 울산대학교 Ai융합전공, 2024년 4학년 1학기, Ai 정보이론 / 수업 내용 및 추가 학습 서술

# Mutual information

: 정보량 관점에서 서로 다른 두 확률 변수가 **서로 얼마나 연관되어 있는지**를 나타낸다.
: 두 확률 분포의 결합 확률분포와 두 확률분포의 곱 간의 KL Divergence로 계산된다.

- 두 확률 분포가 독립이라면 결합 확률분포가 두 확률분포 간의 곱과 같아짐
- 따라서 독립일 경우, KL Divergence가 0이 되고, 그렇지 않다면 0보다 큰 어떤 값을 가지게 된다.
- 두 확률 분포가 독립이 아닌 경우, 두 확률 분포가 서로 연관 되어 갖는 상호정보량을 아래와 같은 식으로 측정하는 것

### Definition

- X와 Y를 joint PMF p(x,y), marginal PMF p(x)와 p(y)를 갖는 두 개의 확률 변수라고 한다.
- 상호 정보 I(X;Y)는 p(x,y)와 p(x)p(y) 사이의 상대 엔트로피이다.
  ![alt text](<Information Theory Attached file/Pasted image 20240325195525.png>)

### Mutual Information Expressed by Entropy

![alt text](<Information Theory Attached file/Pasted image 20240325195740.png>)

- 따라서 mutual information은 Y의 지식으로 인한 X의 불확실성 감소이다.

### Theorem 2.4.1

![alt text](<Information Theory Attached file/Pasted image 20240325195831.png>)

![alt text](<Information Theory Attached file/Pasted image 20240325195904.png>)

# Conditional Mutual Information

- Z가 주어졌을 때, X와 Y의 조건부 상호정보는 아래 식으로 정의된다.
- ![alt text](<Information Theory Attached file/Pasted image 20240325201155.png>)

# Chain Rule for Information

![alt text](<Information Theory Attached file/Pasted image 20240325201333.png>)
![alt text](<Information Theory Attached file/Pasted image 20240325201351.png>)
