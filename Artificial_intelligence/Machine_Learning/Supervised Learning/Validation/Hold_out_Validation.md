# π€ ν•™μµ ν‚¤μ›λ“

## μ •μ

- Hold-out Validation : κ²€μ¦ μ„ΈνΈ μ ‘κ·Όλ²•(Validation Set Approach) μ΄λΌκ³ λ„ λ¶λ¦¬λ©°, ν†µκ³„μ  ν•™μµ λ¨λΈμ ν…μ¤νΈ μ¤λ¥(Test Error)λ¥Ό μ¶”μ •ν•λ” λ° μ‚¬μ©λλ” κ°„λ‹¨ν•κ³  λ„λ¦¬ μ“°μ΄λ” λ°©λ²•

## ν‚¤μ›λ“

- μ΄ λ°©λ²•μ—μ„λ” μ‚¬μ© κ°€λ¥ν• λ°μ΄ν„°μ…‹μ„ λ‘κ°μ λ¶€λ¶„μΌλ΅ λ‚λλ‹¤.

---

# π“ Hold-out Validation

## ν•µμ‹¬ κ³Όμ •

1. λ°μ΄ν„°μ…‹μ„ λλ¤ν•κ² train μ„ΈνΈμ™€ validation μ„ΈνΈλ΅ λ¶„ν• 
2. train μ„ΈνΈλ΅ λ¨λΈμ„ ν•™μµ
3. ν•™μµλ λ¨λΈμ„ κ²€μ¦ μ„ΈνΈμ—μ„ ν‰κ°€ν•λ©°, ν‰κ°€ μ§€ν‘λ¥Ό μ‚¬μ©
4. κ²€μ¦ μ„ΈνΈμ—μ„ μΈ΅μ •λ μ¤μ°¨(error)λ” λ¨λΈμ ν…μ¤νΈ μ¤μ°¨(test error)λ¥Ό μ¶”μ •ν•λ” λ° μ‚¬μ©λλ©°, λ¨λΈμ μΌλ°ν™” μ„±λ¥μ„ ν‰κ°€ν•λ” κΈ°μ¤€μ΄ λλ‹¤.

   ```python
   import numpy as np
   from sklearn.model_selection import train_test_split
   from sklearn.datasets import load_iris
   from sklearn.ensemble import RandomForestClassifier
   from sklearn.metrics import accuracy_score

   # 1. λ°μ΄ν„° λ΅λ“ (Iris λ°μ΄ν„°μ…‹ ν™μ©)
   iris = load_iris()
   X, y = iris.data, iris.target

   # 2. λ°μ΄ν„° λ¶„ν•  (80% ν›λ ¨, 20% κ²€μ¦)
   X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

   # 3. λ¨λΈ ν•™μµ (λλ¤ ν¬λ μ¤νΈ λ¶„λ¥κΈ°)
   model = RandomForestClassifier(n_estimators=100, random_state=42)
   model.fit(X_train, y_train)

   # 4. λ¨λΈ ν‰κ°€ (κ²€μ¦ μ„ΈνΈ ν™μ©)
   y_pred = model.predict(X_val)
   accuracy = accuracy_score(y_val, y_pred)

   print(f"π“ Hold-Out κ²€μ¦ μ •ν™•λ„: {accuracy:.4f}")
   ```

## μ¥λ‹¨μ 

- μ¥μ 
  - κ°„λ‹¨ν•κ³  κµ¬ν„μ΄ μ‰¬μ›€
  - ν›λ ¨ μ¤μ°¨λ³΄λ‹¤ ν…μ¤νΈ μ¤μ°¨λ¥Ό λ” κ°κ΄€μ μΌλ΅ ν‰κ°€ κ°€λ¥
- λ‹¨μ 
  - λ°μ΄ν„° λ¶„ν•  λ°©μ‹μ— λ”°λΌ λ¨λΈ μ„±λ¥ ν‰κ°€κ°€ λ‹¬λΌμ§ μ μμ (μ¦‰, κ²€μ¦ μ„ΈνΈμ μ„ νƒμ΄ κ²°κ³Όμ— ν° μν–¥μ„ μ¤)
  - ν›λ ¨ λ°μ΄ν„°κ°€ μ¤„μ–΄λ“¤κΈ° λ•λ¬Έμ— λ¨λΈμ΄ μµμ μ μ„±λ¥μ„ λ‚΄κΈ° μ–΄λ ¤μΈ μ μμ

---

# β¨ λ‹¤λ¥Έ κ²€μ¦ λ°©λ²•

- LOOCV (Leave-one-out Cross-validation)
  - λ°μ΄ν„°λ¥Ό ν• κ°μ λ°μ΄ν„° ν¬μΈνΈλ¥Ό κ²€μ¦ μ„ΈνΈλ΅ μ‚¬μ©ν•κ³  λ‚λ¨Έμ§€λ¥Ό ν›λ ¨ μ„ΈνΈλ΅ μ‚¬μ©ν•μ—¬ μ—¬λ¬ λ² λ°λ³µ
  - λ¶„μ‚°μ€ μ¤„μ–΄λ“¤μ§€λ§ κ³„μ‚° λΉ„μ©μ΄ νΌ
- K-fold Cross-validation
  - λ°μ΄ν„°λ¥Ό kκ°μ κ· λ“±ν• λ¶€λ¶„(folds)μΌλ΅ λ‚λ„κ³ , (k-1)κ°μ ν΄λ“λ΅ ν›λ ¨ν• ν›„ λ‚λ¨Έμ§€ 1κ° ν΄λ“λ΅ κ²€μ¦
  - μ΄λ¥Ό kλ² λ°λ³µν•μ—¬ λ³΄λ‹¤ μ•μ •μ μΈ μ„±λ¥ ν‰κ°€ κ°€λ¥

# π’»ν™μ© μ‚¬λ΅€

## ν™λ°ν• μ‚¬μ© λ¶„μ•Ό

- λ°μ΄ν„°μ…‹μ΄ ν° κ²½μ°μ— μ μ©ν•¨
- λ¨Έμ‹ λ¬λ‹ λ¨λΈ μ„ νƒ λ° ν•μ΄νΌ νλΌλ―Έν„° νλ‹μ— μμ£Ό μ‚¬μ©λ¨

---

# π”—λ νΌλ°μ¤

## μ°Έκ³  κ°•μ/κΈ€

- [Hold-out vs. Cross-validation in Machine Learning](https://medium.com/@jaz1/holdout-vs-cross-validation-in-machine-learning-7637112d3f8f)
