# 🚀 학습 키워드

## 키워드

- `각각의 층은 어떻게 구성되었고, 또 각 층의 역할은 무엇인가?`
- 가중치, 가중합, 전달함수, 활성화 함수, 손실 함수

---

# 📝새로 배운 개념

## Deep Learning

- 여러 층을 가진 인공신경망을 사용하여 학습을 수행하는 것

## 용어 (Terminology)

|              구분              |  구성요소   |                                                  설명                                                   |
| :----------------------------: | :---------: | :-----------------------------------------------------------------------------------------------------: |
|               층               |   입력층    |                                         데이터를 받아들이는 층                                          |
|               -                |   은닉층    | 모든 입력 노드부터 입력 값을 받아 가중합을 계산하고 이 값을 활성화 함수에 적용하여 출력층에 전달하는 층 |
|               -                |   출력층    |                                    신경망의 최종 결괏값이 포함된 층                                     |
|             가중치             |      -      |                                        노드와 노드 간 연결 강도                                         |
|            바이어스            |      -      | 가중합에 더해 주는 상수, 하나의 뉴런에서 활성화 함수를 거쳐 최종적으로 출력되는 값을 조절하는 역할을 함 |
| 가중합(weighted sum), 전달함수 |      -      |                                      가중치와 신호의 곱을 합한 것                                       |
|              함수              | 활성화 함수 |                          신호를 입력받아 이를 적절히 처리하여 출력해 주는 함수                          |
|               -                |  손실함수   |                 가중치 학습을 위해 출력 함수의 결과와 실제 값 간의 오차를 측정하는 함수                 |

## 가중치 (Weight)

- 입력 값이 연산 결과에 미치는 영향력을 조절하는 요소
- 역할 : 모델이 입력 데이터에서 학습한 패턴을 저장
- 예시 : $w_1$ 값이 0 혹은 0과 가까운 0.001이라면, $x_1$이 아무리 큰 값이라도 $x_1*w_1$ 값은 0이거나 0에 가까운 값이 된다. 이와 같이 입력 값의 연산 결과를 조정하는 역할을 하는 것이 가중치이다.

## 가중합 또는 전달함수 (Weighted Sum)

- 가중합 = 전달함수
- 각 노드에서 들어오는 신호에 가중치를 곱해서 다음 노드로 전달 -> `이 값들을 모두 더한 합계를 가중합`
  - 또한, 노드의 가중합이 계산되면 이 가중합을 활성화 함수로 보내기 때문에 `전달함수(Transfer function)`라고도 한다.
  - $\sum_{i=1}^{n} x_i*w_i + b$

## 활성화 함수 (Activation Function)

- 전달 받은 값을 출력할 때 일정 기준에 따라 출력값을 변화시키는 **비선형함수**
- 종류 : **시그모이드 (Sigmoid), 하이퍼볼릭 탄젠트 (Tanh), 렐루 (ReLU) 함수 등**

### 시그모이드 함수 (Sigmoid Function)

- 선형 함수의 결과를 0 ~ 1 사이에서 비선형 형태로 변형
- 주로 로지스틱 회귀와 같은 분류 문제를 확률적으로 표현하는 데 사용
- 과거에는 인기가 많았으나, 모델의 깊이가 깊어지면서 기울기가 사라지는 `기울기 소멸 문제(Vanishing Gradient Problem)`가 발생하여 현재는 잘 사용되지 않음

### 하이퍼볼릭 탄젠트 함수 (Hyperbolic Tangent Function)

- 선형 함수의 결과를 -1 ~ 1 사이에서 비선형 형태로 변형
- 시그모이드에서 결과값이 평균이 0이 아닌 양수로 편향된 문제를 해결하는데 사용했지만, 기울기 소멸 문제는 여전히 발생

### 렐루 함수 (ReLU, Rectified Linear Unit)

- 입력( $x$ )이 음수일 때는 0을 출력, 양수일 때는 $x$ 를 출력
- 경사하강법에 영향을 주지 않아 학습속도가 빠르고, 기울기 소멸 문제가 발생하지 않는 장점
- 일반적으로 은닉층에서 사용, 하이퍼볼릭 탄젠트 함수 대비 학습속도가 6배 빠르다.
  - 문제는 음수 값을 입력받으면 항상 0을 출력하기 때문에 학습 능력이 감소, 이를 해결하려고 `리키 렐루(Leaky ReLU)` 함수등을 사용

### 리키 렐루 함수 (Leaky ReLU)

- 입력이 음수일 때 0이 아닌 0.001처럼 매우 작은 값을 반환
- ReLU의 문제점인 입력 값이 수렴하는 구간이 제거되는 문제 해결

### 소프트맥스 함수 (Softmax Function)

- 입력 값을 0~1 사이에 출력되도록 정규화하여 출력 값들의 총합이 항상 1이 되도록 한다.
- 보통 딥러닝 출력노드의 활성화 함수로 많이 사용
  - $y_k = \frac{e(a_k)}{\sum_{i=1}^{n} e(a_i)}$
  - $exp(x)$ 는 `지수 함수(Exponential Function)`로, 입력값을 지수로 변환하는 함수
- $n$ 은 출력층의 뉴런 개수, $y_k$ 는 그 중 $k$ 번째 출력을 의미
- 즉, 이 수식처럼 소프트맥스 함수의 분자는 입력 신호 $a_k$ 의 지수함수, 분모는 모든 입력 신호의 지수함수 합으로 구성

### 렐루 함수와 소프트맥스 함수 구현

    ```python
    class Net(torch.nn.Module):
        def __init__(self, n_feature, n_hidden, n_output):
            super(Net, self).__init__()
            self.hidden = torch.nn.Linear(n_feature, n_hidden) #------ 은닉층
            self.relu = torch.nn.ReLu(inplace=True)
            self.out = torch.nn.Linear(n_hidden, n_output) #------ 출력층
            self.softmax = torch.nn.Softmax(dim=n_output)
        def forward(self, x):
            x = self.hidden(x)
            x = self.relu(x) #------ 은닉층을 위한 렐루 활성화 함수
            x = self.out(x)
            x = self.softmax(x) #------ 출력층을 위한 소프트맥스 활성화 함수
            return x
    ```

## 손실 함수 (Loss Function)

- 경사하강법 : 학습률과 손실 함수의 순간 기울기를 이용하여 가중치를 업데이트 하는 방법
  - 즉, 미분의 기울기를 이용하여 오차를 비교하고 최소화하는 방향으로 이동시키는 방법이라고 할 수 있다.
  - 이때, 오차를 구하는 방법이 손실함수이다.
- `-> 즉, 손실함수는 학습을 통해 얻은 데이터의 추정치가 실제 데이터와 얼마나 차이가 나는지 평가하는 지표`
- 손실함수 값이 클수록 많이 틀렸다는 의미, '0'에 가까울수록 완벽하게 추정할 수 있다는 의미
- 종류 : **평균 제곱 오차(Mean Squared Error, MSE), 교차 엔트로피 오차(Cross Entropy Error, CEE) 등**

### 평균 제곱 오차 (Mean Squared Error, MSE)

- 실제 값과 예측값의 차이(error)를 제곱하여 평균을 낸 것
- 실제 값과 예측 값의 차이가 클수록 평균 제곱 오차의 값도 커진다는 것 -> `반대로 생각하면 이 값이 작을수록 예측력이 좋다는 것을 의미`
  - 수식 : $MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2$
    - $y_i$ : 정답 레이블, $\hat{y_i}$ : 신경망의 출력(신경망이 추정한 값)
    - $i$ : 데이터의 차원 개수
  ```python
  loss_fn = torch.nn.MSELoss(reduction='sum')
  y_pred = model(x)
  loss = loss_fn(y_pred, y)
  ```

### 교차 엔트로피 오차 (Cross Entropy Error, CEE)

- 분류 문제에서 원-핫 인코딩 했을 때만 사용할 수 있는 오차 계산법
- 일반적으로 분류 문제에서는 데이터의 출력을 0과 1로 구분하기 위해 시그모이드 함수를 사용,
  - 시그모이드 함수에 포함된 자연 상수 $e$ 때문에 평균 제곱 오차를 적용하면 매끄럽지 못한 그래프 (울퉁불퉁한 그래프) 가 출력
  - 따라서 크로스 엔트로피 손실 함수를 사용하는 데 , 이 손실 함수를 적용할 경우 경사하강법과정에서 학습이 지역최소점에서 멈출 수 있다.
    - **이것을 방지하고자 자연 상수 `e`에 반대되는 자연로그를 모델의 출력 값에 취한다.**
  - 수식 : $CEE = -\sum_{i=1}^{n} y_i log(\hat{y_i})$
    - $y_i$ : 정답 레이블, $\hat{y_i}$ : 신경망의 출력(신경망이 추정한 값)
    - $i$ : 데이터의 차원 개수
  ```python
  loss = nn.CrossEntropyLoss()
  input = torch.randn(5, 6, requires_grad=True) #------ torch.randn은 평균이 0이고 표준편차가 1인 가우시안 정규분포를 이용하여 숫자를 생성
  target = torch.empty(3, dtype=torch.long).random_(5) #------ torch.empty는 dtype torch.float32의 랜덤한 값으로 채워진 텐서를 반환
  output = loss(input, target)
  output.backward()
  ```

---

# 🔗레퍼런스

## 참고 강의/글

- [딥러닝 파이토치 교과서]
