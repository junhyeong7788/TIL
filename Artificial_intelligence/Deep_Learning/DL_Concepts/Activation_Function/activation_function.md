# 🚀 Activation Function

### 🎯 주요 키워드

`활성화 함수 (Activation Function)` : 인공신경망(Artificial Neural Network)에서 입력 신호를 출력 신호로 변환하는 **비선형 함수**

- 뉴런의 활성 상태를 결정하는 역할, 신경망이 단순한 선형 변환을 넘어서 복잡한 데이터 패턴을 학습할 수 있도록 하는 핵심 요소
- 만약 활성화 함수가 없거나 단순한 선형 함수라면, 심층 신경망이 단순 선형 회귀와 같은 성능을 보일 수 있음

---

# 📝 새로 배운 개념

## 🏷 활성화 함수의 역할

- 비선형 변환 제공 : 입력과 출력 사이의 비선형 관계를 형성하여 신경망이 복잡한 패턴을 학습할 수 있도록 해줌
- 특징 학습 (Feature Learning) : 네트워크가 원시 입력에서 유용한 특징을 추출할 수 있도록 도와줌
- 다층 신경망의 의미 부여 : 다층 구조의 신경망이 단순한 선형 회귀 모델과 차별화될 수 있도록 함
- 출력 제한 및 안정성 제공 : 일부 활성화 함수는 출력을 특정 범위로 제한하여 학습을 안정화 시킴

### 비선형 함수여야 하는 이유

1. 선형 활성화 함수는 다층 구조를 무의미하게 만듦
2. 복잡한 함수 근사 가능 : 다층 신경망은 **Universal Approximation Theorem(보편 근사 정리)** 에 따라 임의의 연속 함수를 원하는 정도로 근사 가능
   - 즉, 충분한 뉴런을 갖춘 단일 은닉층 네트워크는 어떤 복잡한 함수라도 근사 가능
3. 특징 공간 변형 및 패턴 인식 가능 : XOR문제를 해결하려면 선형적으로 분리가 불가능한 데이터를 비선형적으로 변형해야함
4. 상호작용 효과와 고차원 특징 표현 가능 : 신경망이 단순한 선형 관계뿐만 아니라 변수 간의 복잡한 상호작용을 학습할 수 있도록 도와줌

## 🏷 활성화 함수의 종류

- **시그모이드 함수 (Sigmoid Function)**
- **하이퍼볼릭 탄젠트 함수 (Hyperbolic Tangent Function)**
- **렐루 함수 (ReLU, Rectified Linear Unit)**
- **리키 렐루 함수 (Leaky ReLU)**
- **소프트맥스 함수 (Softmax Function)**

## 🏷 활성화 함수 선택 시 고려사항

- 기울기 소실 문제 : 딥러닝에서는 시그모이드, 탄젠트 함수보다 ReLU 및 변형 활성화 함수가 선호
- 계산 효율성 : ReLU는 단순한 연산을 제공, GPU연산 최적화에 유리
- 출력 중심성 : Tanh는 평균이 0에 가까워 학습이 빠를 수 있음
- 실험적 성능 : Swish와 같은 최신 함수들은 특정 데이터셋에서 향상된 성능을 보이기도 함

---

# 🔗 레퍼런스

## 참고 강의/글

- [딥러닝 파이토치 교과서]
