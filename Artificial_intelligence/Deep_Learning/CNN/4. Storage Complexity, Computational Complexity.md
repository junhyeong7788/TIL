# Feature Map Size

- Stride 값을 늘리면?
	- 연산량 감소
	- 출력 Feature의 수 감소
	- 정보 손실 발생
- padding 값이 1보다 큰 값으로 설정 하면?
	- feature의 수 감소를 막을 수 있다.

# Storage Complexity

### Weight 수
- layer i 의 가중치 수 : 학습을 통해 업데이트 되는 파라미터 수 = ( k * k * c<sub>i-1</sub> ) * c<sub>i</sub>
	- k : filter size
	- C<sub>i-1</sub> : 이전 레이어의 채널의 수
	- C<sub>i</sub> : 현재 레이어의 채널의 수
- 신경망 학습 이후 메모리에 저장되는 값
	- Storage Complexity = weight 값들이 모두 메모리에 저장됨

### memory
- layer i의 메모리 = 학습 결과 = ( m<sub>i</sub>  * m<sub>i</sub>  ) * c<sub>i</sub> 
	- m<sub>i</sub> : 현재 레이어의 feature map 사이즈
	- c<sub>i</sub> : 현재 레이어의 채널의 수
- 추론 과정에서의 메모리가 소요되는 정도
	- run time

# Computational Complexity
- layer i의 계산 복잡도 = ( k * k ) * ( m<sub>i</sub> * m<sub>i</sub> ) * ( c<sub>i-1</sub> * c<sub>i</sub> )
	- k : filter size
	- m<sub>i</sub> : 현재 레이어의 feature map 사이즈
	- C<sub>i-1</sub> : 이전 레이어의 채널의 수
	- C<sub>i</sub> : 현재 레이어의 채널의 수
- 추론 과정에서 계산 복잡도 결정
	- 학습과정에는 GPU등 하드웨어 리소스 사용가능, 그러므로 계산 복잡도는 큰 고려요소가 아님