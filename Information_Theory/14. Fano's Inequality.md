> 울산대학교 Ai융합전공, 2024년 4학년 1학기, Ai 정보이론 / 수업 내용 및 추가 학습 서술

# Formulation

- 우리는 p(x) 분포를 갖는 RV X를 *추정*하기를 원한다.
- 우리는 p(y|x)를 갖는 X와 관련된 RV Y를 관찰한다.
- Y로부터 정의역의 추정값 내의 X의 추정치인 g(Y) =X의 추정값을 계산한다.

# 확률변수의 추정오류
### 추정(Estimation)
: 관측되지 않은 값을 기반으로 확률변수의 값을 예측하는 과정
- 예시 : 한 도시의 평균 온도를 예측하기 위해 일정한 기간 동안의 온도데이터를 사용할 수 있다.
- 이때 *평균 온도는 확률변수*로 모델링되며, 실제로 관측한 데이터를 바탕으로 그 확률변수의 값을 추정하게 된다.

### 추정오류(Estimation Error)
: 실제 확률변수의 값과 추정된 값 사이의 차이
- 오류를 정량화하는 방법은 여러가지가 있지만, 가장 일반적인 방법은 *평균 제곱 오류(MSE)* 를 사용하는 것
- MSE는 *추정값과 실제값의 차이를 제곱한 것*의 평균으로 오류의 크기를 수치적으로 나타낸다.

### 평균제곱 오류
:  `MSE=E[(X^−X)^2]`
- 여기서 X^는 X의 추정값이며, `E[*]`는 기댓값(평균)을 의미

### 예시
: 어떤 학급의 평균 키를 추정, 이 학급의 실제 평균 키가 160cm라고 가정
- 학생들의 키를 임의로 측정하여 계산한 평균 키가 158cm라면, 추정오류는 2cm이다.
- 만약 추정 과정을 여러 번 반복한다면 MSE는 추정된 키와 실제 키의 차이를 제곱한 값들의 평균을 제공

# Goal

![alt text](<Information Theory Attached file/Pasted image 20240407214355.png>)
: 추정이나 분류 문제에서 오류 확률의 하한을 결정하는 데 사용된다.
- 확률 변수의 추정 오류와 엔트로피 사이의 관계를 나타냄
- "오류 확률의 하한"이라는 용어는 특정 조건 하에서 가능한 최소 오류 확률을 의미
- 이 하한은 보통 어떤 추정이나 분류 문제에 대해 이론적으로 증명 될 수 있는 값으로, 모든 가능한 방법 중 최적의 방법을 사용했을 때도 이 오류률이하로 떨어뜨릴 수 없다는 것을 나타냄
# Fano's Inequality

### Theorem
- 확률변수 X와 Y가 있고, Y를 관측하여 X를 추정하려 할 때, X의 추정값을 X^라고 한다.
![alt text](<Information Theory Attached file/Pasted image 20240407214526.png>)
: 원래의 부등식을 단순화 시켜 오류 확률 P_e와 조건부엔트로피 H(X|Y)사이의 관계를 더 명확하게 나타낸다
- Pe​는 추정 오류 확률입니다.
- 𝐻(𝑋∣𝑌)는 주어진 𝑌에 대한 𝑋의 조건부 엔트로피입니다.
- ∣𝑋∣는 𝑋가 취할 수 있는 값들의 개수(클래스의 수)입니다.
- log⁡2는 로그의 밑이 2인 로그 함수입니다.

- H(P_e)가 최대 1비트의 정보만을 가질 수 있음을 고려하여 단순화된 형태이다.
- 이진 엔트로피의 성질 : 오류 확률 P_e에 대한 이진 엔트로피 H(P_e)는 P_e가 0 또는 1에 가까울 때 0에 가까워지고, P_e = 0.5일 때 최댓값 1을 가진다.

## 파노의 부등식의 의미
: 조건부 엔트로피 H(X|Y)가 작을 수록 추정오류확률 P_e가 작아질 수 있다는 것
- 조건부 엔트로피 H(X|Y)는 Y를 알고 있을때 X의 불확실성을 수치화한 것으로, Y가 X에 대해 많은 정보를 제공할수록 H(X|Y)는 작아진다.
- 예시 : 질병 진단 테스트, X는 질병의 유무, Y는 테스트결과를 바탕으로 질병의 유무(X)를 추정(X^)하게 된다. 만약 테스트가 매우 정확하다면, Y는 X에 대해 많은 정보를 제공하고, H(X|Y)는 매우 낮을 것이다.
	- 이 경우 파노의 부등식에 따라 Pe도 낮을 것이다.


### 증명 1

![alt text](<Information Theory Attached file/Pasted image 20240414215641.png>)
![alt text](<Information Theory Attached file/Pasted image 20240414215937.png>)

![alt text](<Information Theory Attached file/Pasted image 20240414215951.png>)
![alt text](<Information Theory Attached file/Pasted image 20240414220002.png>)

# Consequences

- Fano's inequality와 그에 따른 결론

![alt text](<Information Theory Attached file/Pasted image 20240414220226.png>)

# IID ( Independent and Identically Distributed)
: 여러 확률 변수들이 서로 독립적이며, 같은 확률 분포를 따른다는 것을 의미

### 독립성
: 두 확률변수 X와 Y가 독립적이라는 것은 한 변수의 결과가 다른 변수의 결과에 영향을 주지 않는다는 의미
- `P(X=x and Y=y)=P(X=x)×P(Y=y)`
- 이 관계가 모든 x와 y에 대해 성립하면, X와 Y는 독립적이다.

### 동일분포 ( Identically Distributed)
: 여러 확률변수들이 같은 확률 분포를 갖는다는 것을 의미 
- 확률변수 𝑋1,𝑋2,…,𝑋𝑛​이 모두 평균 𝜇와 분산 𝜎2를 갖는 정규분포 𝑁(𝜇,𝜎^2)를 따른다면,
- 이 변수들은 동일 분포를 갖는다고 말함

### 독립동일 분포
: 확률변수들이 독립동일분포를 따른다면, 이들 각각은 서로에게 영향을 주지 않으며, 같은 확률분포에서 추출된다. 

### 예시
 1. 주사위 던지기 : 공정한 주사위를 여러 번 던지는 경우, 각 던짐은 서로 독립적이며 모든 결과는 1부터 6까지의 숫자 중 하나를 동일한 확률로 보여줍니다. 이 경우 각 주사위 던지기는 독립동일분포를 따릅니다.
2. 서베이 샘플링: 사람들의 의견을 묻는 설문조사에서 각 응답자가 무작위로 선택되고, 모든 응답자가 설문 문항에 대해 독립적으로 응답한다면, 이 데이터는 i.i.d.로 모델링될 수 있습니다.

# IID RVs

: 독립 동일 분포를 따르는 두 확률 변수 X와 X'에 대한 보조정리이다.

![alt text](<Information Theory Attached file/Pasted image 20240414220345.png>)

- 균일 분포가 가장 높은 엔트로피를 가지고 있으며, 확률변수가 같은 확률이 가장 낮다는 것을 의미한다.
- 증명 : jensen's 부등식 사용, 볼록 함수에 대해 기댓값이 함수의 기댓값보다 크거나 같다는 것을 말함.

![alt text](<Information Theory Attached file/Pasted image 20240414220543.png>)

- 결과적으로 확률변수의 엔트로피가 높을수록, 즉 확률변수가 더 많은 불확실성을 가질수록, 두 확률변수가 동일한 값을 취할 확률이 낮아진다는 것을 수학적으로 증명한다.

# Independent RVs

![alt text](<Information Theory Attached file/Pasted image 20240414220932.png>)
: X는 확률밀도함수 p(x)를, X'는 확률밀도함수 r(x)를 갖는다.

- 두 확률변수가 같은 값을 가질 확률과 이들의 엔트로피 및 상대 엔트로피사이의 관계를 제시한다.

### 증명

![alt text](<Information Theory Attached file/Pasted image 20240414221128.png>)
![alt text](<Information Theory Attached file/Pasted image 20240414221133.png>)

- 독립적인 확률변수의 분포가 서로 다를 때, 그 차이를 상대엔트로피를 통해 어느정도 측정할 수 있음을 나타낸다.
- 그리고 두 확률변수가 같은 값을 가질 확률의 하한을 상대엔트로피를 사용하여 추정할 수 있다.
- 두 독립적인 시스템이 같은 결과를 내놓을 확률을 평가할 때 유용할 수 있다.
