> 울산대학교 Ai융합전공, 2024년 4학년 1학기, Ai 정보이론 / 수업 내용 및 추가 학습 서술

# Mutual information

상호정보량
: 정보량 관점에서 서로 다른 두 확률 변수 X와 Y가 **서로 얼마나 연관되어 있는지**를 나타낸다.
: 두 확률 분포의 결합 확률분포와 두 확률분포의 곱 간의 KL Divergence로 계산된다.

- 쉽게 말하면, X의 값을 알면 Y의 값에 대한 *예측 가능성이 얼마나 증가하는지*를 나타낸다.

- 두 확률 분포가 **독립이라면**, 결합 확률분포가 두 확률분포 간의 곱과 같아짐 (X를 알고 있더라도 Y의 결과에 대한 예측 가능성은 변하지 않는다)

  - 따라서 독립일 경우, KL Divergence가 0이 된다.
  - 증명 : I(X; Y) = H(X, Y) - H(X) - H(Y) = 0

- 두 확률 분포가 **독립이 아닌 경우**, 두 확률 분포가 서로 연관 되어 갖는 상호정보량을 아래와 같은 식으로 측정하는 것 (X를 알고 있을 때 Y의 결과에 대한 예측 가능성이 증가한다.)
  - 이 경우 상호정보량은 0보다 큰 값을 갖는다.
  - 증명 : I(X; Y) > 0
  - 예시 : 첫 번째 동전이 앞면이었다면 두 번째 동전이 앞면일 확률이 높아진다

### Definition

- X와 Y를 joint PMF p(x,y), marginal PMF p(x)와 p(y)를 갖는 두 개의 확률 변수라고 한다.
- 상호 정보 I(X;Y)는 p(x,y)와 p(x)p(y) 사이의 상대 엔트로피이다.
  ![alt text](<Information Theory Attached file/Pasted image 20240325195525.png>)
- I(X; Y): X와 Y의 상호 정보량
- p(xᵢ, yⱼ): X가 xᵢ이고 Y가 yⱼ일 확률
- p(xᵢ): X가 xᵢ일 확률
- p(yⱼ): Y가 yⱼ일 확률

### Mutual Information Expressed by Entropy

![alt text](<Information Theory Attached file/Pasted image 20240325195740.png>)

- 상호 정보량은 X와 Y의 **결합 엔트로피** (H(X, Y))에서 X의 **엔트로피** (H(X))와 Y의 **조건부 엔트로피** (H(Y|X))를 뺀 값으로 계산됩니다.
- 즉, 상호 정보량은 X를 알고 있을 때 Y의 불확실성이 얼마나 감소하는지를 나타냅니다.

### Mutual information이 고려하는 요소

1. Y의 지식으로 인한 X의 불확실성 감소:  Y의 값을 알면 X의 값에 대한 예측 가능성이 증가할 수 있습니다.
2. X의 지식으로 인한 Y의 불확실성 감소: X의 값을 알면 Y의 값에 대한 예측 가능성이 증가할 수도 있습니다.
3. X와 Y의 상호 의존성: X와 Y가 서로 독립적인 경우, mutual information은 0입니다. 하지만 X와 Y가 서로 관련되어 있는 경우, mutual information은 0보다 큰 값을 갖습니다.

- 예시:
  - 동전을 두 번 던지는 예시를 생각해 보겠습니다.
    - **X:** 첫 번째 동전의 결과 (앞면 또는 뒷면)
    - **Y:** 두 번째 동전의 결과 (앞면 또는 뒷면)
  - 만약 두 동전의 결과가 서로 독립적이라면:
    - Y의 지식은 X의 불확실성을 감소시키지 않습니다.
    - 즉, mutual information은 0입니다.
  - 하지만 만약 두 동전의 결과가 서로 관련되어 있다면:
    - Y의 지식은 X의 불확실성을 감소시킬 수 있습니다.
    - 예를 들어, 첫 번째 동전이 앞면이었다면 두 번째 동전이 앞면일 확률이 높아집니다.
    - 이 경우, mutual information은 0보다 큰 값을 갖습니다.

![alt text](<Information Theory Attached file/Pasted image 20240410232035.png>)
![alt text](<Information Theory Attached file/Pasted image 20240410232016.png>)
- I(X;Y) = H(X,Y)-H(X)-H(Y|X)
### Theorem 2.4.1

![alt text](<Information Theory Attached file/Pasted image 20240325195831.png>)

![alt text](<Information Theory Attached file/Pasted image 20240325195904.png>)

# Chain Rule for Entropy

### Theorem 2.5.1

![alt text](<Information Theory Attached file/Pasted image 20240327043805.png>)
![alt text](<Information Theory Attached file/Pasted image 20240327043842.png>)

- 체인 규칙은 엔트로피를 계산하는 방법을 제공한다.
- 엔트로피는 정보의 불확실성을 측정하는 지표이다.
- 체인 규칙은 엔트로피를 조건부 엔트로피의 합으로 표현
- 조건부 엔트로피는 한 변수의 엔트로피를 다른 변수가 주어졌을 때 계산한 값이다.
- 체인 규칙은 엔트로피를 계산하는 데 유용한 도구이다.

- 증명
- 체인 규칙 증명:
  (1) X₁, X₂, ..., Xₙ을 n개의 확률 변수라고 정의
  (2) H(X₁, X₂, ..., Xₙ)을 계산하기 위해 다음과 같이 분해
  ```
  H(X₁, X₂, ..., Xₙ) = H(X₁) + H(X₂ | X₁) + ... + H(Xₙ | Xₙ₋₁, ..., X₁)
  ```
  (3) H(X₂ | X₁)을 계산
  ```
  H(X₂ | X₁) = -Σᵢⱼ p(xᵢ, xⱼ) * log(p(xⱼ | xᵢ))
  ```
  (4) p(xⱼ | xᵢ)를 다음과 같이 분해
  ```
  p(xⱼ | xᵢ) = p(xⱼ, xᵢ) / p(xᵢ)
  ```
  (5) p(xⱼ, xᵢ)를 다음과 같이 분해
  ```
  p(xⱼ, xᵢ) = p(x₁, x₂, ..., xₙ)
  ```
  (6) 위의 식을 (3)번 식에 대입
  ```
  H(X₂ | X₁) = -Σᵢⱼ p(x₁, x₂, ..., xₙ) * log(p(xⱼ | xᵢ)) / p(xᵢ)
  ```
  (7) (2)번 식에 (6)번 식을 대입
  ```
  H(X₁, X₂, ..., Xₙ) = H(X₁) - Σᵢⱼ p(x₁, x₂, ..., xₙ) * log(p(xⱼ | xᵢ)) / p(xᵢ) + ... + H(Xₙ | Xₙ₋₁, ..., X₁)
  ```
  (8) 위의 식을 정리하면 다음과 같다.
  ```
  H(X₁, X₂, ..., Xₙ) = Σᵢ=₁ⁿ H(Xᵢ | Xᵢ₋₁, ..., X₁)
  ```

### 예시

- 동전을 두 번 던지는 경우, 각 던지기의 결과는 서로 독립적이다.
- 따라서 두 던지기의 엔트로피는 다음과 같다.

```
H(X₁, X₂) = H(X₁) + H(X₂) = 1 + 1 = 2
```

- 하지만, 첫 번째 던지기의 결과가 주어졌을 때 두 번째 던지기의 엔트로피는 다음과 같다.

```
H(X₂ | X₁) = 0
```

- 따라서 체인 규칙을 사용하면 다음과 같이 엔트로피를 계산할 수 있다.

```
H(X₁, X₂) = H(X₁) + H(X₂ | X₁) = 1 + 0 = 1
```

# Conditional Mutual Information

- 두 확률 변수 X와 Y가 있을 때, Z가 주어졌을 때 X의 엔트로피를 H(X | Z)라고 정의
- H(X | Z) = -Σᵢⱼ p(xᵢ, zⱼ) \* log(p(xᵢ | zⱼ))
- ![alt text](<Information Theory Attached file/Pasted image 20240325201155.png>)
- 조건부 상호 정보량은 X와 Y가 서로 얼마나 많은 정보를 공유하는지를 나타낸다.
- 조건부 상호 정보량은 0에서 ∞까지의 값을 가질 수 있습니다.
- 조건부 상호 정보량이 0이면 X와 Y는 서로 독립적입니다.
- 조건부 상호 정보량이 ∞이면 X와 Y는 완전히 의존적입니다.

- 증명
  ![alt text](<Information Theory Attached file/Pasted image 20240411020855.png>)

# Chain Rule for Information

![alt text](<Information Theory Attached file/Pasted image 20240325201333.png>)

- **왼쪽:** 엔트로피 H(X₁, X₂, ..., Xₙ)를 계산하는 문제를 제시합니다.
- **오른쪽:** 체인 규칙을 이용하여 엔트로피를 계산하는 과정을 보여줍니다.
  ![alt text](<Information Theory Attached file/Pasted image 20240325201351.png>)
- n개의 확률 변수 X₁, X₂, ..., Xₙ의 엔트로피 H(X₁, X₂, ..., Xₙ)는 다음과 같이 계산됩니다.
- H(X₁, X₂, ..., Xₙ) = Σᵢ=₁ⁿ H(Xᵢ | Xᵢ₋₁, ..., X₁)

- 엔트로피를 직접 계산하는 대신, 조건부 엔트로피를 계산하여 합산하는 방식
- 조건부 엔트로피는 한 변수의 엔트로피를 다른 변수가 주어졌을 때 계산한 값이다.

### 예시

- 동전을 두 번 던지는 경우, 각 던지기의 결과는 서로 독립적입니다.
- 따라서 체인 규칙을 이용하여 엔트로피를 다음과 같이 계산할 수 있습니다.

```
H(X₁, X₂) = H(X₁) + H(X₂ | X₁) = 1 + 1 = 2
```
