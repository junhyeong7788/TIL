> 울산대학교 Ai융합전공, 2024년 4학년 1학기, Ai 정보이론 / 수업 내용 및 추가 학습 서술

# Log Sum Inequality

- 로그합 : 두 개이상의 로그 값을 더하는 연산
- a_1, a_2, ... , a_n and b_1, b_2, ... , b_n은 음수가 아니다. (비음수)
- 비음수 : 0보다 크거나 같은 수를 의미 - 즉, 음수가 아닌 모든 실수를 포함한다. (양수, 0, 소수) - 특징 : 더하거나 곱하면 항상 비음수, 0으로 나누면 정의되지 않는다, 제곱근은 항상 비음수

- 두 확률분포 p(x)와 q(x)가 주어졌을 때, 모든 x에 대해 p(x)>0 및 q(x)>=0일때, 다음 부등식이 성립
  ![alt text](<Information Theory Attached file/Pasted image 20240407184007.png>)
- 부등식이 성립하는 조건은 모든 a_i, b_i가 일정한 경우에만 동등하게 적용된다. 즉, 같을 때 성립.
- jensen부등식의 등호 조건은 f(x)가 직선 함수일 때 이다. 즉, 모든 a_i\* b_i가 같아야한다.
- 우변은 두 확률 분포의 총 확률질량의 비율에 대한 로그의 가중 평균이다.

- 각각의 a_i​와 b_i​ 사이의 비율을 로그 함수에 적용한 후 이를 a_i​로 가중 평균한 것이 전체 a_i​의 합에 대한 로그 함수의 가중 평균보다 항상 크거나 같다는 것을 나타낸다.
	- 로그 함수의 볼록성 때문에 이런 부등식이 성립한다.
### 예시

- a_1 = 1, a_2 = 2, b_1 = 3, b_2 = 4라고 가정하면:

```
log(1 * 3 + 2 * 4) = log(14) >= log(1 * 3) + log(2 * 4)
= log(3) + log(8)
```

## 증명

- 일반성을 잃지않고, a_i > 0이고 b_i > 0이라고 가정 (WLOG (Without Loss of Generality) 가정을 사용)
  ![alt text](<Information Theory Attached file/Pasted image 20240411034030.png>) - 여기서 불평등은 젠슨의 부등식으로부터 오는것, ![{\displaystyle {\frac {b*{i}}{b}}\geq 0}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3425e4e6b6fd95bca5b2ba696a37094cab11a6f1), ![{\displaystyle \sum _{i=1}^{n}{\frac {b_{i}}{b}}=1}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3122681e020df75cdab4bcdc97f31df90c69c159) 그리고 f는 볼록함수이다. - f(t) = t log t는 엄밀하게 볼록인 점에 유의

# Nonnegativity of Relative Entropy (상대 엔트로피의 비음성)

- 상대 엔트로피의 비음성성을 증명하는데 사용되는 로그합 불평등을 보여준다.
  ![alt text](<Information Theory Attached file/Pasted image 20240407184530.png>)
- p와 q는 모두 PMF이므로 p(x)/q(x) = 1인 경우에만 등식이 유지됩니다.
- 확률의 합은 1이다.

1. jensen 부등식에 의해 `E[f(X)] ≥ f(E[X])` 을 적용
2. f(x) = log(x)는 볼록함수임을 알수있다.
3. `E[log(X)] >= log(E[X])`
4. `E[X] = Σ_i p(x) * x = Σ_i p(x) = 1` 이다.
5. 따라서 `D(p||q) >= 0` 이다.

# Convexity of Relative Entropy

: 상대 엔트로피의 볼록성

- D(p||q)는 pair (p,q)에서 볼록이며, 즉 임의의
  ![alt text](<Information Theory Attached file/Pasted image 20240411035022.png>)
- 에 대하여
  ![alt text](<Information Theory Attached file/Pasted image 20240407210804.png>)
- 이고 여기서 (p_1, q_1)와 (p_2,q_2)는 두 쌍의 PMF이다.
- **D(λp1 + (1-λ)p2 || λq1 + (1-λ)q2)** 는 **λ** 에 대해 볼록 함수이다.

- **λ**는 0과 1 사이의 값
- **P1**과 **Q1**은 첫 번째 확률 분포와 참조 확률 분포
- **P2**과 **Q2**은 두 번째 확률 분포와 참조 확률 분포
- **D(λP1 + (1-λ)P2 || λQ1 + (1-λ)Q2)** 는 두 확률 분포의 선형 조합 사이의 상대 엔트로피이다.

## 증명

- 로그 합 부등식을 적용하여 다음과 같은 결과를 얻을 수 있다.
  ![alt text](<Information Theory Attached file/Pasted image 20240407211001.png>)
- 이를 x 전체에 합산하면 주장이 나옵니다.

# Concavity of Entropy (엔트로피의 오목성)

: 정보이론에서 엔트로피의 오목성을 보여준다.
: 함수 H(p)는 p의 오목 함수이다.

## 증명

![alt text](<Information Theory Attached file/Pasted image 20240411040359.png>)

- 여기서 u는 균등 분포입니다.
- H의 오목성은 D의 볼록성으로부터 바로 이어진다.
- `D(p || u)`는 p와 u 사이의 쿨백-라이블러 발산, 볼록 함수임을 증명한다.
- 따라서 `-D(p || u)`는 오목 함수, 따라서 `log |X| - D(p || u)`도 오목 함수

![alt text](<Information Theory Attached file/Pasted image 20240411040411.png>)

- 조건화는 엔트로피를 감소시킨다. 즉, `H(X | Y) <= H(X)` 다음과 같다.

```
Z = { X1 with probability λ
	  X2 with probability 1 - λ }
```

- Z는 정의된 임의 변수, λ는 0과 1 사이의 실수
- 그러면 Z의 분포는 `λp1 + (1 - λ)p2`와 같다.
- 위 `H(Z) >= H(Z | θ)` 로 증명, 여기서 θ는 Z의 분포를 결정하는 변수이다.
- 따라서 `H(λp1 + (1 - λ)p2) >= λH(p1) + (1 - λ)H(p2)`과 같다.
  - 이것은 엔트로피가 분포의 오목 함수임을 증명한다.
